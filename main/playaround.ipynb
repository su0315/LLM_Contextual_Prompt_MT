{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import re\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from functools import partial\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file1_path=\"/mnt/data-poseidon/sumire/thesis/2-1/en-ja/Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-2-1/prompt+source.txt\"\n",
    "file2_path = \"/mnt/data-poseidon/sumire/thesis/2-1/en-ja/Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-2-1/source.txt\"\n",
    "label_path = \"/mnt/data-poseidon/sumire/thesis/2-1/en-ja/Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-2-1/comet_binary.txt\"\n",
    "\n",
    "\n",
    "def concatenate_lines(file1_path, file2_path):\n",
    "    # Read the lines from both files\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        lines1 = file1.readlines()\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        lines2 = file2.readlines()\n",
    "\n",
    "    # Concatenate lines from the two files to create instances\n",
    "    instances =  [ line1.strip() + line2.strip() for line1, line2 in zip(lines1, lines2)]\n",
    "\n",
    "    return instances\n",
    "# Load and concatenate the data\n",
    "instances = concatenate_lines(file1_path, file2_path)\n",
    "\n",
    "# Label load\n",
    "all_labels = []\n",
    "with open (label_path, 'r')as file:\n",
    "    for line in file:\n",
    "        score = int(line.strip())\n",
    "        all_labels.append(score)\n",
    "\n",
    "# Define the index where you want to split the dataset\n",
    "split_index = 100  # Replace this with your desired index\n",
    "\n",
    "# Split the custom_dataset into training and validation\n",
    "train_dataset = Dataset.from_dict({'text': instances[:split_index], 'label':all_labels[:split_index]})\n",
    "validation_dataset = Dataset.from_dict({\"text\": instances[split_index:], 'label':all_labels[split_index:]})\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "validation_dataset = validation_dataset.shuffle(seed=42)\n",
    "# Access the training and validation datasets\n",
    "print(\"Training dataset:\")\n",
    "print(len(train_dataset))\n",
    "\n",
    "#print(\"Validation dataset:\")\n",
    "print(len(validation_dataset))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized_datasets = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = train_tokenized_datasets.select(range(10)) ##########\n",
    "small_eval_dataset = val_tokenized_datasets.select(range(10)) #######\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    do_eval= True,\n",
    "    do_predict= True,\n",
    "    do_train=True, \n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred, metric):\n",
    "    _ = eval_pred \n",
    "    #logits, labels = eval_pred\n",
    "    predictions, labels\n",
    "    print(eval_pred)\n",
    "    #print (logits, labels)\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    #accuracy = accuracy_score(y_true = labels, y_pred=predictions)\n",
    "    \n",
    "    #accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    result = {\"accuracy\": 2}\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=partial(compute_metrics,metric)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "trainer.evaluate()\n",
    "#preds, label_ids, metrics = trainer.predict(small_eval_dataset)##############put test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/data-poseidon/sumire/thesis/running/ted/eval_mt/train-val/testing_Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-1-1/prompt+source.txt\"\n",
    "# Read your input file\n",
    "with open(path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use regular expressions to find and extract the sentences after \"Statement A\"\n",
    "matches = re.findall(r'Given context:\\n(.*?)\\nTranslate English to Japanese:', text, re.DOTALL)\n",
    "\n",
    "# Join the matched sentences within each \"Statement A\" with commas\n",
    "joined_sentences = [''.join(match.strip().split('\\n')) for match in matches]\n",
    "\n",
    "# Join the matched sentences into a single string\n",
    "extracted_text = '\\n'.join(joined_sentences)\n",
    "\n",
    "# Save the extracted text into a new file\n",
    "with open('extracted.txt', 'w') as output_file:\n",
    "    output_file.write(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sentence is here!?yeeees!It's not like they're mutually exclusive or anything.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import re\n",
    "file1_path=\"/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/extracted.txt\"\n",
    "file2_path = \"/mnt/data-poseidon/sumire/thesis/running/ted/eval_mt/train-val/testing_Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-1-1/source.txt\"\n",
    "\n",
    "def concatenate_lines(file1_path, file2_path):\n",
    "    # Read the lines from both files\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        lines1 = file1.readlines()\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        lines2 = file2.readlines()\n",
    "\n",
    "    # Concatenate lines from the two files to create instances\n",
    "    instances =  [ line1.strip() + line2.strip() for line1, line2 in zip(lines1, lines2)]\n",
    "\n",
    "    return instances\n",
    "\n",
    "# Load and concatenate the data\n",
    "instances = concatenate_lines(file1_path, file2_path)\n",
    "\n",
    "# Access the dataset\n",
    "print(instances[200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Training dataset:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "['Context sentence is here!?yeeees!When you do break it, I\\'m going to give you a hundred bucks.\"', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Now, of course, this was a simple ploy on Dr. P's part  to get me to do the exercises I didn't want to do  before the prospect of being the richest five-year-old in the second floor ward,  but what he effectively did for me was reshape an awful daily occurrence  into a new and promising experience for me.\", 'Context sentence is here!?yeeees!and his declaration of me as a strong and powerful little girl  shaped my own view of myself  as an inherently strong, powerful and athletic person well into the future.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This is an example of how adults in positions of power  can ignite the power of a child.', \"Context sentence is here!?yeeees!But, in the previous instances of those thesaurus entries,  our language isn't allowing us to evolve into the reality that we would all want,  the possibility of an individual to see themselves as capable.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Our language hasn't caught up with the changes in our society,  many of which have been brought about by technology.\", 'Context sentence is here!?yeeees!Certainly, from a medical standpoint,  my legs, laser surgery for vision impairment,  titanium knees and hip replacements for aging bodies  that are allowing people to more fully engage with their abilities,  and move beyond the limits that nature has imposed on them --  not to mention social networking platforms  allow people to self-identify, to claim their own descriptions of themselves,  so they can go align with global groups of their own choosing.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So, perhaps technology is revealing more clearly to us now  what has always been a truth:  that everyone has something rare and powerful to offer our society,  and that the human ability to adapt is our greatest asset.', \"Context sentence is here!?yeeees!The human ability to adapt, it's an interesting thing,  because people have continually wanted to talk to me about overcoming adversity,  and I'm going to make an admission:  This phrase never sat right with me,  and I always felt uneasy trying to answer people's questions about it,  and I think I'm starting to figure out why.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Implicit in this phrase of \"overcoming adversity\"  is the idea that success, or happiness,  is about emerging on the other side of a challenging experience  unscathed or unmarked by the experience,  as if my successes in life have come about from an ability  to sidestep or circumnavigate the presumed pitfalls of a life with prosthetics,  or what other people perceive as my disability.', 'Context sentence is here!?yeeees!But, in fact, we are changed. We are marked, of course, by a challenge,  whether physically, emotionally or both.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And I'm going to suggest that this is a good thing.\", \"Context sentence is here!?yeeees!Adversity isn't an obstacle that we need to get around  in order to resume living our life.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It's part of our life.\", 'Context sentence is here!?yeeees!And I tend to think of it like my shadow.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Sometimes I see a lot of it, sometimes there's very little,  but it's always with me.\", \"Context sentence is here!?yeeees!And, certainly, I'm not trying to diminish the impact, the weight, of a person's struggle.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There is adversity and challenge in life,  and it's all very real and relative to every single person,  but the question isn't whether or not you're going to meet adversity,  but how you're going to meet it.\", 'Context sentence is here!?yeeees!So, our responsibility is not simply shielding those we care for from adversity,  but preparing them to meet it well.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And we do a disservice to our kids  when we make them feel that they're not equipped to adapt.\", \"Context sentence is here!?yeeees!There's an important difference and distinction  between the objective medical fact of my being an amputee  and the subjective societal opinion of whether or not I'm disabled.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And, truthfully, the only real and consistent disability I've had to confront  is the world ever thinking that I could be described by those definitions.\", \"Context sentence is here!?yeeees!In our desire to protect those we care about  by giving them the cold, hard truth about their medical prognosis,  or, indeed, a prognosis on the expected quality of their life,  we have to make sure that we don't put the first brick in a wall  that will actually disable someone.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Perhaps the existing model of only looking at what is broken in you  and how do we fix it, serves to be more disabling to the individual  than the pathology itself.', 'Context sentence is here!?yeeees!By not treating the wholeness of a person,  by not acknowledging their potency,  we are creating another ill on top of whatever natural struggle they might have.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=We are effectively grading someone's worth to our community.\", 'Context sentence is here!?yeeees!So we need to see through the pathology  and into the range of human capability.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And, most importantly, there's a partnership  between those perceived deficiencies and our greatest creative ability.\", \"Context sentence is here!?yeeees!So it's not about devaluing, or negating, these more trying times  as something we want to avoid or sweep under the rug,  but instead to find those opportunities wrapped in the adversity.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So maybe the idea I want to put out there is  not so much overcoming adversity  as it is opening ourselves up to it,  embracing it, grappling with it,  to use a wrestling term,  maybe even dancing with it.', \"Context sentence is here!?yeeees!And, perhaps, if we see adversity as natural, consistent and useful,  we're less burdened by the presence of it.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This year we celebrate the 200th birthday of Charles Darwin,  and it was 150 years ago, when writing about evolution,  that Darwin illustrated, I think, a truth about the human character.', \"Context sentence is here!?yeeees!To paraphrase: It's not the strongest of the species that survives,  nor is it the most intelligent that survives;  it is the one that is most adaptable to change.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Conflict is the genesis of creation.', \"Context sentence is here!?yeeees!From Darwin's work, amongst others, we can recognize that  the human ability to survive and flourish  is driven by the struggle of the human spirit through conflict  into transformation.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So, again, transformation, adaptation, is our greatest human skill.', \"Context sentence is here!?yeeees!And, perhaps, until we're tested, we don't know what we're made of.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Maybe that's what adversity gives us:  a sense of self, a sense of our own power.\", 'Context sentence is here!?yeeees!So, we can give ourselves a gift.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=We can re-imagine adversity as something more than just tough times.', 'Context sentence is here!?yeeees!Maybe we can see it as change.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Adversity is just change that we haven't adapted ourselves to yet.\", \"Context sentence is here!?yeeees!I think the greatest adversity that we've created for ourselves  is this idea of normalcy.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Now, who's normal?\", \"Context sentence is here!?yeeees!There's no normal.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There's common, there's typical. There's no normal,  and would you want to meet that poor, beige person if they existed?\", \"Context sentence is here!?yeeees!I don't think so.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If we can change this paradigm from one of achieving normalcy  to one of possibility -- or potency, to be even a little bit more dangerous --  we can release the power of so many more children,  and invite them to engage their rare and valuable abilities with the community.', 'Context sentence is here!?yeeees!Anthropologists tell us that the one thing  we as humans have always required of our community members  is to be of use, to be able to contribute.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There's evidence that Neanderthals, 60,000 years ago,  carried their elderly and those with serious physical injury,  and perhaps it's because the life experience of survival of these people  proved of value to the community.\", \"Context sentence is here!?yeeees!They didn't view these people as broken and useless;  they were seen as rare and valuable.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=A few years ago, I was in a food market in the town where I grew up  in that red zone in northeastern Pennsylvania,  and I was standing over a bushel of tomatoes.', 'Context sentence is here!?yeeees!It was summertime: I had shorts on.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I hear this guy, his voice behind me say, \"Well, if it isn\\'t Aimee Mullins.\"', \"Context sentence is here!?yeeees!And I turn around, and it's this older man. I have no idea who he is.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And I said, \"I\\'m sorry, sir, have we met? I don\\'t remember meeting you.\"', 'Context sentence is here!?yeeees!He said, \"Well, you wouldn\\'t remember meeting me.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I mean, when we met I was delivering you from your mother\\'s womb.\"', 'Context sentence is here!?yeeees!Oh, that guy.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And, but of course, actually, it did click.', \"Context sentence is here!?yeeees!This man was Dr. Kean,  a man that I had only known about through my mother's stories of that day,  because, of course, typical fashion, I arrived late for my birthday by two weeks.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so my mother's prenatal physician had gone on vacation,  so the man who delivered me was a complete stranger to my parents.\", 'Context sentence is here!?yeeees!And, because I was born without the fibula bones,  and had feet turned in, and a few toes in this foot and a few toes in that,  he had to be the bearer -- this stranger had to be the bearer of bad news.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=He said to me, \"I had to give this prognosis to your parents  that you would never walk,  and you would never have the kind of mobility that other kids have  or any kind of life of independence,  and you\\'ve been making liar out of me ever since.\"', 'Context sentence is here!?yeeees!The extraordinary thing is that he said he had saved  newspaper clippings throughout my whole childhood,  whether winning a second grade spelling bee,  marching with the Girl Scouts, you know, the Halloween parade,  winning my college scholarship, or any of my sports victories,  and he was using it, and integrating it into teaching resident students,  med students from Hahnemann Medical School and Hershey Medical School.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And he called this part of the course the X Factor,  the potential of the human will. No prognosis can account for how powerful this could be  as a determinant in the quality of someone's life.\", 'Context sentence is here!?yeeees!And Dr. Kean went on to tell me,  he said, \"In my experience, unless repeatedly told otherwise,  and even if given a modicum of support,  if left to their own devices, a child will achieve.\"', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=See, Dr. Kean made that shift in thinking.', \"Context sentence is here!?yeeees!He understood that there's a difference between the medical condition  and what someone might do with it.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And there's been a shift in my thinking over time,  in that, if you had asked me at 15 years old,  if I would have traded prosthetics for flesh-and-bone legs,  I wouldn't have hesitated for a second.\", 'Context sentence is here!?yeeees!I aspired to that kind of normalcy back then.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But if you ask me today, I'm not so sure.\", \"Context sentence is here!?yeeees!And it's because of the experiences I've had with them,  not in spite of the experiences I've had with them.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And perhaps this shift in me has happened  because I've been exposed to more people who have opened doors for me  than those who have put lids and cast shadows on me.\", 'Context sentence is here!?yeeees!See, all you really need is one person  If you can hand somebody the key to their own power --  the human spirit is so receptive -- if you can do that  and open a door for someone at a crucial moment,  you are educating them in the best sense.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=You're teaching them to open doors for themselves.\", 'Context sentence is here!?yeeees!In fact, the exact meaning of the word \"educate\"  comes from the root word \"educe.\"', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It means \"to bring forth what is within,  to bring out potential.\"', 'Context sentence is here!?yeeees!So again, which potential do we want to bring out?', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There was a case study done in 1960s Britain,  when they were moving from grammar schools to comprehensive schools.', 'Context sentence is here!?yeeees!It\\'s called the streaming trials. We call it \"tracking\" here in the States.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It's separating students from A, B, C, D and so on.\", 'Context sentence is here!?yeeees!And the \"A students\" get the tougher curriculum, the best teachers, etc.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Well, they took, over a three-month period,  D-level students, gave them A\\'s,  told them they were \"A\\'s,\" told them they were bright,  and at the end of this three-month period,  And, of course, the heartbreaking, flip side of this study,  is that they took the \"A students\" and told them they were \"D\\'s.\"', \"Context sentence is here!?yeeees!And that's what happened at the end of that three-month period.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=A crucial part of this case study was that the teachers were duped too.', \"Context sentence is here!?yeeees!The teachers didn't know a switch had been made.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=They were simply told, \"These are the \\'A-students,\\' these are the \\'D-students.\\'\"  And that\\'s how they went about teaching them and treating them.', \"Context sentence is here!?yeeees!So, I think that the only true disability is a crushed spirit,  a spirit that's been crushed doesn't have hope,  it doesn't see beauty,  it no longer has our natural, childlike curiosity  and our innate ability to imagine.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If instead, we can bolster a human spirit to keep hope,  to see beauty in themselves and others,  to be curious and imaginative,  then we are truly using our power well.', 'Context sentence is here!?yeeees!When a spirit has those qualities, we are able to create new realities  and new ways of being.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I\\'d like to leave you with a poem  by a fourteenth-century Persian poet named Hafiz  that my friend, Jacques Dembois told me about,  and the poem is called \"The God Who Only Knows Four Words\":  \"Every child has known God,  not the God of names,  not the God of don\\'ts,  but the God who only knows four words and keeps repeating them,  saying, \\'Come dance with me.', 'Context sentence is here!?yeeees!Come, dance with me. Come, dance with me.\\'\"  Thank you.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This is a representation of your brain,  and your brain can be broken into two parts.', \"Context sentence is here!?yeeees!There's the left half, which is the logical side,  and then the right half,  which is the intuitive.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so if we had a scale to measure the aptitude of each hemisphere,  then we can plot our brain.', \"Context sentence is here!?yeeees!And for example, this would be somebody who's completely logical.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This would be someone who's entirely intuitive.\", 'Context sentence is here!?yeeees!So where would you put your brain on this scale?', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Some of us may have opted for one of these extremes,  but I think for most people in the audience,  your brain is something like this --  with a high aptitude in both hemispheres at the same time.', \"Context sentence is here!?yeeees!It's not like they're mutually exclusive or anything.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=You can be logical and intuitive.', 'Context sentence is here!?yeeees!And so I consider myself one of these people,  along with most of the other experimental quantum physicists,  who need a good deal of logic  to string together these complex ideas.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But at the same time, we need a good deal of intuition  to actually make the experiments work.', 'Context sentence is here!?yeeees!How do we develop this intuition? Well we like to play with stuff.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So we go out and play with it, and then we see how it acts,  and then we develop our intuition from there.', 'Context sentence is here!?yeeees!And really you do the same thing.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So some intuition  that you may have developed over the years  is that one thing is only in one place at a time.', \"Context sentence is here!?yeeees!I mean, it can sound weird to think about  one thing being in two different places at the same time,  but you weren't born with this notion, you developed it.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And I remember watching a kid playing on a car stop.', \"Context sentence is here!?yeeees!He was just a toddler and he wasn't very good at it, and he kept falling over.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But I bet playing with this car stop taught him a really valuable lesson,  and that's that large things don't let you get right past them,  and that they stay in one place.\", \"Context sentence is here!?yeeees!And so this is a great conceptual model to have of the world,  unless you're a particle physicist.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It'd be a terrible model for a particle physicist,  because they don't play with car stops,  they play with these little weird particles.\", 'Context sentence is here!?yeeees!And when they play with their particles,  they find they do all sorts of really weird things --  like they can fly right through walls,  or they can be in two different places at the same time.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so they wrote down all these observations,  and they called it the theory of quantum mechanics.', \"Context sentence is here!?yeeees!And so that's where physics was at a few years ago;  to describe little, tiny particles.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But you didn't need it  to describe the large, everyday objects around us.\", \"Context sentence is here!?yeeees!This didn't really sit well with my intuition,  and maybe it's just because I don't play with particles very often.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Well, I play with them sometimes,  but not very often.', \"Context sentence is here!?yeeees!And I've never seen them.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I mean, nobody's ever seen a particle.\", \"Context sentence is here!?yeeees!But it didn't sit well with my logical side either.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Because if everything is made up of little particles  and all the little particles  follow quantum mechanics,  then shouldn't everything just follow quantum mechanics?\", \"Context sentence is here!?yeeees!I don't see any reason why it shouldn't.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=if we could somehow show  that an everyday object  also follows quantum mechanics.', 'Context sentence is here!?yeeees!So a few years ago, I set off to do just that.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So I made one.', 'Context sentence is here!?yeeees!This is the first object  that has been in a mechanical quantum superposition.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So what we're looking at here  is a tiny computer chip.\", 'Context sentence is here!?yeeees!And you can sort of see this green dot right in the middle.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And that's this piece of metal I'm going to be talking about in a minute.\", 'Context sentence is here!?yeeees!This is a photograph of the object.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And here I'll zoom in a little bit. We're looking right there in the center.\", \"Context sentence is here!?yeeees!And then here's a really, really big close-up of the little piece of metal.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So what we're looking at is a little chunk of metal,  and it's shaped like a diving board, and it's sticking out over a ledge.\", 'Context sentence is here!?yeeees!And so I made this thing  in nearly the same way as you make a computer chip.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I went into a clean room with a fresh silicon wafer,  and then I just cranked away at all the big machines for about 100 hours.', 'Context sentence is here!?yeeees!For the last stuff, I had to build my own machine --  to make this swimming pool-shaped hole  underneath the device.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This device has the ability  to be in a quantum superposition,  but it needs a little help to do it.', 'Context sentence is here!?yeeees!Here, let me give you an analogy.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=You know how uncomfortable it is to be in a crowded elevator?', \"Context sentence is here!?yeeees!I mean, when I'm in an elevator all alone, I do all sorts of weird things,  but then other people get on board  and I stop doing those things  because I don't want to bother them,  or, frankly, scare them.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So quantum mechanics says  that inanimate objects feel the same way.', \"Context sentence is here!?yeeees!The fellow passengers for inanimate objects  are not just people,  but it's also the light shining on it  and the wind blowing past it and the heat of the room.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so we knew, if we wanted to see  this piece of metal behave quantum mechanically,  we're going to have to kick out all the other passengers.\", \"Context sentence is here!?yeeees!And so that's what we did.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=We turned off the lights,  and then we put it in a vacuum and sucked out all the air,  to just a fraction of a degree above absolute zero.', 'Context sentence is here!?yeeees!Now, all alone in the elevator,  the little chunk of metal is free to act however it wanted.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so we measured its motion.', 'Context sentence is here!?yeeees!We found it was moving in really weird ways.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Instead of just sitting perfectly still, it was vibrating,  and the way it was vibrating was breathing something like this --  like expanding and contracting bellows.', \"Context sentence is here!?yeeees!And by giving it a gentle nudge,  we were able to make it both vibrate  and not vibrate  something that's only allowed with quantum mechanics.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So what I'm telling you here is something truly fantastic.\", 'Context sentence is here!?yeeees!What does it mean for one thing  to be both vibrating and not vibrating  at the same time?', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So let's think about the atoms.\", 'Context sentence is here!?yeeees!So in one case:  all the trillions of atoms that make up that chunk of metal  are sitting still  and at the same time those same atoms  are moving up and down.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Now it's only at precise times when they align.\", \"Context sentence is here!?yeeees!The rest of the time they're delocalized.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=That means that every atom  is in two different places at the same time,  which in turn means the entire chunk of metal  is in two different places.', 'Context sentence is here!?yeeees!I think this is really cool.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Really.', 'Context sentence is here!?yeeees!It was worth locking myself in a clean room to do this for all those years  because, check this out,  the difference in scale  between a single atom and that chunk of metal  is about the same as the difference  between that chunk of metal and you.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So if a single atom can be in two different places at the same time,  that chunk of metal can be in two different places,  then why not you?', 'Context sentence is here!?yeeees!I mean, this is just my logical side talking.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So imagine if you're in multiple places at the same time,  what would that be like?\", 'Context sentence is here!?yeeees!How would your consciousness  handle your body being delocalized in space?', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There's one more part to the story.\", \"Context sentence is here!?yeeees!It's when we warmed it up,  and we turned on the lights and looked inside the box,  we saw that the piece metal was still there in one piece.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And so I had to develop this new intuition,  that it seems like all the objects in the elevator  are really just quantum objects  just crammed into a tiny space.', 'Context sentence is here!?yeeees!You hear a lot of talk  about how quantum mechanics says that everything is all interconnected.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Well, that's not quite right.\", \"Context sentence is here!?yeeees!It's more than that; it's deeper.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It's that those connections,  your connections to all the things around you,  literally define who you are,  and that's the profound weirdness of quantum mechanics.\", 'Context sentence is here!?yeeees!Thank you.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I'm Jessi, and this is my suitcase.\", \"Context sentence is here!?yeeees!But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I love finding, wearing,  and more recently,  photographing and blogging  a different, colorful, crazy outfit  for every single occasion.', \"Context sentence is here!?yeeees!But I don't buy anything new.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I get all my clothes secondhand  from flea markets and thrift stores.', 'Context sentence is here!?yeeees!Aww, thank you.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Secondhand shopping allows me to reduce the impact  my wardrobe has on the environment  and on my wallet.', 'Context sentence is here!?yeeees!I get to meet all kinds of great people;  my dollars usually go to a good cause;  I look pretty unique;  and it makes shopping like my own personal treasure hunt.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I mean, what am I going to find today?', 'Context sentence is here!?yeeees!Is it going to be my size?', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Will I like the color?', 'Context sentence is here!?yeeees!Will it be under $20?', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If all the answers are yes,  I feel as though I've won.\", 'Context sentence is here!?yeeees!I want to get back to my suitcase  and tell you what I packed  for this exciting week here at TED.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I mean, what does somebody with all these outfits bring with her?', \"Context sentence is here!?yeeees!So I'm going to show you exactly what I brought.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I brought seven pairs of underpants  and that's it.\", \"Context sentence is here!?yeeees!Exactly one week's worth of undies  is all I put in my suitcase.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I was betting that I'd be able to find  everything else I could possible want to wear  once I got here to Palm Springs.\", \"Context sentence is here!?yeeees!And since you don't know me  as the woman walking around TED in her underwear --    that means I found a few things.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And I'd really love to show you my week's worth of outfits right now.\", 'Context sentence is here!?yeeees!Does that sound good?', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So as I do this,  I'm also going to tell you a few of the life lessons  in these adventures wearing nothing new.\", \"Context sentence is here!?yeeees!So let's start with Sunday.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I call this \"Shiny Tiger.\"', 'Context sentence is here!?yeeees!You do not have to spend a lot of money to look great.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=You can almost always look phenomenal for under $50.', 'Context sentence is here!?yeeees!This whole outfit, including the jacket,  cost me $55,  and it was the most expensive thing  that I wore the entire week.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Monday: Color is powerful.', \"Context sentence is here!?yeeees!to be in a bad mood when you're wearing bright red pants.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If you are happy,  you are going to attract other happy people to you.', 'Context sentence is here!?yeeees!Tuesday:  Fitting in is way overrated.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I've spent a whole lot of my life  trying to be myself  and at the same time fit in.\", 'Context sentence is here!?yeeees!Just be who you are.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If you are surrounding yourself with the right people,  they will not only get it,  they will appreciate it.', 'Context sentence is here!?yeeees!Wednesday:  Embrace your inner child.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Sometimes people tell me  that I look like I'm playing dress-up,  or that I remind them of their seven-year-old.\", 'Context sentence is here!?yeeees!I like to smile  and say, \"Thank you.\"', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Thursday:  Confidence is key.', 'Context sentence is here!?yeeees!If you think you look good in something,  you almost certainly do.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And if you don't think you look good in something,  you're also probably right.\", 'Context sentence is here!?yeeees!I grew up with a mom who taught me this day-in and day-out.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But it wasn't until I turned 30  that I really got what this meant.\", \"Context sentence is here!?yeeees!And I'm going to break it down for you for just a second.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=If you believe you're a beautiful person  there is no look that you can't pull off.\", 'Context sentence is here!?yeeees!So there is no excuse for any of us here in this audience.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=We should be able to rock anything we want to rock.', 'Context sentence is here!?yeeees!Thank you.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Friday: A universal truth -- five words for you:  Gold sequins go with everything.', 'Context sentence is here!?yeeees!And finally, Saturday:  Developing your own unique personal style  without having to say a word.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It's been proven to me time and time again  as people have walked up to me this week  simply because of what I'm wearing,  and we've had great conversations.\", 'Context sentence is here!?yeeees!So obviously this is not all going to fit  back in my tiny suitcase.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=So before I go home to Brooklyn,  I'm going to donate everything back.\", \"Context sentence is here!?yeeees!Because the lesson I'm trying to learn myself this week  is that it's okay to let go.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=I don't need to get emotionally attached to these things  because around the corner,  there is always going to be  another crazy, colorful,  shiny outfit  just waiting for me,  if I put a little love in my heart and look.\", 'Context sentence is here!?yeeees!Thank you very much.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Thank you.', 'Context sentence is here!?yeeees!Everybody talks about happiness these days.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=with \"happiness\" in the title published in the last five years  and they gave up after about 40, and there were many more.', 'Context sentence is here!?yeeees!There is a huge wave of interest in happiness,  among researchers.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There is a lot of happiness coaching.', 'Context sentence is here!?yeeees!Everybody would like to make people happier.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=But in spite of all this flood of work,  there are several cognitive traps  that sort of make it almost impossible to think straight  about happiness.', 'Context sentence is here!?yeeees!And my talk today will be mostly about these cognitive traps.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=This applies to laypeople thinking about their own happiness,  and it applies to scholars thinking about happiness,  because it turns out we're just as messed up as anybody else is.\", 'Context sentence is here!?yeeees!The first of these traps  is a reluctance to admit complexity.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It turns out that the word \"happiness\"  is just not a useful word anymore,  because we apply it to too many different things.', \"Context sentence is here!?yeeees!I think there is one particular meaning to which we might restrict it,  but by and large,  this is something that we'll have to give up  and we'll have to adopt the more complicated view  of what well-being is.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=The second trap is a confusion between experience and memory;  basically, it's between being happy in your life,  and being happy about your life  or happy with your life.\", \"Context sentence is here!?yeeees!And those are two very different concepts,  and they're both lumped in the notion of happiness.\", \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And the third is the focusing illusion,  and it's the unfortunate fact that we can't think about any circumstance  that affects well-being  without distorting its importance.\", 'Context sentence is here!?yeeees!I mean, this is a real cognitive trap.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=There's just no way of getting it right.\", \"Context sentence is here!?yeeees!Now, I'd like to start with an example  of somebody who had a question-and-answer session after one of my lectures reported a story,  and that was a story --  He said he'd been listening to a symphony,  and it was absolutely glorious music  and at the very end of the recording,  there was a dreadful screeching sound.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And then he added, really quite emotionally,  it ruined the whole experience.', \"Context sentence is here!?yeeees!But it hadn't.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=What it had ruined were the memories of the experience.', 'Context sentence is here!?yeeees!He had had the experience.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=He had had 20 minutes of glorious music.', 'Context sentence is here!?yeeees!They counted for nothing  because he was left with a memory;  the memory was ruined,  and the memory was all that he had gotten to keep.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=What this is telling us, really,  is that we might be thinking of ourselves and of other people  in terms of two selves.', 'Context sentence is here!?yeeees!There is an experiencing self,  who lives in the present  and knows the present,  is capable of re-living the past,  but basically it has only the present.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=It\\'s the experiencing self that the doctor approaches --  you know, when the doctor asks,  \"Does it hurt now when I touch you here?\"', 'Context sentence is here!?yeeees!And then there is a remembering self,  and the remembering self is the one that keeps score,  and maintains the story of our life,  and it\\'s the one that the doctor approaches  in asking the question,  \"How have you been feeling lately?\"', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=or \"How was your trip to Albania?\" or something like that.', 'Context sentence is here!?yeeees!Those are two very different entities,  the experiencing self and the remembering self,  and getting confused between them is part of the mess  about the notion of happiness.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Now, the remembering self  is a storyteller.', 'Context sentence is here!?yeeees!And that really starts with a basic response of our memories --  it starts immediately.', \"Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=We don't only tell stories when we set out to tell stories.\", 'Context sentence is here!?yeeees!Our memory tells us stories,  that is, what we get to keep from our experiences  is a story.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And let me begin with one example.', 'Context sentence is here!?yeeees!This is an old study.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Those are actual patients undergoing a painful procedure.', \"Context sentence is here!?yeeees!I won't go into detail. It's no longer painful these days,  but it was painful when this study was run in the 1990s.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=They were asked to report on their pain every 60 seconds.', 'Context sentence is here!?yeeees!Here are two patients,  those are their recordings.', 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=And you are asked, \"Who of them suffered more?\"', \"Context sentence is here!?yeeees!And it's a very easy question.\", 'Yaaaaaaaaaaaaaaaaaaaaaaaaaaa3221=Clearly, Patient B suffered more --  his colonoscopy was longer,  and every minute of pain that Patient A had,  Patient B had, and more.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3519f4d569450eb1a439f09ae98284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define the index where you want to split the dataset\n",
    "split_index = 100  # Replace this with your desired index\n",
    "\n",
    "# Split the custom_dataset into training and validation\n",
    "train_dataset = Dataset.from_dict({'text': instances[:split_index]})\n",
    "print (train_dataset)\n",
    "validation_dataset = Dataset.from_dict({\"text\": instances[split_index:]})\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'val':  validation_dataset\n",
    "}\n",
    "\n",
    "# Access the training and validation datasets\n",
    "print(\"Training dataset:\")\n",
    "print(dataset['train'])\n",
    "\n",
    "#print(\"Validation dataset:\")\n",
    "print(dataset['val']['text'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "print (tokenized_datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sentence is here!?yeeees!It's not like they're mutually exclusive or anything.\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named val expected length 100 but got length 275",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m (train_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m validation_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_dict({\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: instances[split_index\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\u001b[39mlen\u001b[39m(instances)]})\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_dict({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m: train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m:  validation_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m })\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/datasets/arrow_dataset.py:873\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    868\u001b[0m     mapping \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mencode_batch(mapping)\n\u001b[1;32m    869\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    870\u001b[0m     col: OptimizedTypedSequence(data, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mfeatures[col] \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m }\n\u001b[0;32m--> 873\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict(mapping\u001b[39m=\u001b[39;49mmapping)\n\u001b[1;32m    874\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m Features({col: ts\u001b[39m.\u001b[39mget_inferred_type() \u001b[39mfor\u001b[39;00m col, ts \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/datasets/table.py:787\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    772\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    773\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m        :class:`datasets.table.Table`:\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/pyarrow/table.pxi:3643\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/pyarrow/table.pxi:5170\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/pyarrow/table.pxi:3592\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/pyarrow/table.pxi:2785\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named val expected length 100 but got length 275"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "file1_path=\"/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/extracted.txt\"\n",
    "file2_path = \"/mnt/data-poseidon/sumire/thesis/running/ted/eval_mt/train-val/testing_Llama-2-70b-instruct-v2-usas-zs-p1-nsplit-ja-1-1/source.txt\"\n",
    "def concatenate_lines(file1_path, file2_path):\n",
    "    # Read the lines from both files\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        lines1 = file1.readlines()\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        lines2 = file2.readlines()\n",
    "\n",
    "    # Concatenate lines from the two files to create instances\n",
    "    instances =  [ line1.strip() + line2.strip() for line1, line2 in zip(lines1, lines2)]\n",
    "\n",
    "    return instances\n",
    "\n",
    "# Load and concatenate the data\n",
    "instances = concatenate_lines(file1_path, file2_path)\n",
    "\n",
    "# Access the dataset\n",
    "print(instances[200])\n",
    "\n",
    "# Define the index where you want to split the dataset\n",
    "split_index = 100  # Replace this with your desired index\n",
    "\n",
    "# Split the custom_dataset into training and validation\n",
    "train_dataset = Dataset.from_dict({'text': instances[:split_index]})\n",
    "print (train_dataset)\n",
    "validation_dataset = Dataset.from_dict({\"text\": instances[split_index-1:len(instances)]})\n",
    "\n",
    "dataset = Dataset({\n",
    "    'train': train_dataset,\n",
    "    'val':  validation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d273f727be4e9291a60e4bf3123d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6241ca6c05174362999907c70e5e55cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37a737c542e4657a4707b45e52d6b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset yelp_review_full/yelp_review_full to /home/sumire/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08d36c398104d6c89866dca6a963c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6c1eb03704905a4440eef0e38dc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63930d7aa17549c5aca7580cb05efecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yelp_review_full downloaded and prepared to /home/sumire/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c91a0c516704501bb8691fe1d96464e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1dd77bbe44a15b648ffc50bd8f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/650 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394dd0072b894995ba53bd36aa09b07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: accelerate in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (0.24.0)\n",
      "Requirement already satisfied: pyyaml in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: psutil in /home/sumire/.local/lib/python3.8/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sumire/.local/lib/python3.8/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from accelerate) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from accelerate) (0.16.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/sumire/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (65.5.1)\n",
      "Requirement already satisfied: wheel in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: filelock in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from huggingface-hub->accelerate) (3.8.2)\n",
      "Requirement already satisfied: requests in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2022.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sumire/miniconda3/envs/llm_mt3/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./running\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/transformers/training_args.py:1372\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1367\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1372\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1373\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1374\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1375\u001b[0m ):\n\u001b[1;32m   1376\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1377\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m     )\n\u001b[1;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1389\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/transformers/training_args.py:1795\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1795\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/transformers/training_args.py:1716\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1715\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1716\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   1717\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1718\u001b[0m         )\n\u001b[1;32m   1719\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1720\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 6)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = range(1,5+1)\n",
    "print (a)\n",
    "\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'extractive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mextractive\u001b[39;00m \u001b[39mimport\u001b[39;00m ExtractiveSummarizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/main/playaround.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'extractive'"
     ]
    }
   ],
   "source": [
    "from extractive import ExtractiveSummarizer\n",
    "import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_checkpoint= \"/mnt/data-poseidon/sumire/repos/transformersum/ckpt/epoch=3.ckpt\"\n",
    "model = ExtractiveSummarizer.load_from_checkpoint(model_checkpoint)\n",
    "\n",
    "#model.to(device)  \n",
    "text_to_summarize = \"And keep it clean!Mr. Chandler!My friends, you are all familiar with Beaugard.This is perhaps his masterpiece.\"\n",
    "input_sentences = [\"And keep it clean!\", \"Mr. Chandler!\", \"My friends, you are all familiar with Beaugard.\", \"This is perhaps his masterpiece.\"]\n",
    "summary = model.predict_sentences(input_sentences, raw_scores=False, num_summary_sentences=1)\n",
    "print (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
