
generic:
  data_path: /home/sumire/thesis/LLM_Contextual_Prompt_MT/data/iwslt_hf/
  #src_lang: en
  tgt_lang: ja
  src_context: 2
  #tgt_context: 0
  #dropout: 0.0
  #speaker: False
  #random_context: False
  #tag: False
  #output_dir: "./results/" # Modify here
  batch_size: 4
  model_checkpoint : /mnt/data-poseidon/sumire/hf_llama
  k : 4
  prompt_type : 2
  max_new_tokens : 64 #128 
  max_length : 2048 #512 # 512 # 272 should be okay for k = 3# 313 should be ok for k=4 when context size increased source side is cutted so make it 1024 now
  cfg_name : llama-2048-ja-3-1-p2
