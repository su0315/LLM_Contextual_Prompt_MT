{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d70b1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: /text-generation-inference: No such file or directory\n",
      "/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook\n"
     ]
    }
   ],
   "source": [
    "!cd /home/sumire/text-generation-inference\n",
    "!echo PYTHONPATH\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf6daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd \n",
    "from text_generation import Client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f0fc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:8765\n"
     ]
    }
   ],
   "source": [
    "TGI_CENTRAL_ADDRESS=os.environ.get('TGI_CENTRAL_ADDRESS')\n",
    "print(TGI_CENTRAL_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7baad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'upstage/Llama-2-70b-instruct-v2',\n",
       "  'address': '0.0.0.0:8082',\n",
       "  'owner': 'patrick',\n",
       "  'is_quantized': True},\n",
       " {'name': 'lmsys/vicuna-33b-v1.3',\n",
       "  'address': '0.0.0.0:8081',\n",
       "  'owner': 'patrick',\n",
       "  'is_quantized': True},\n",
       " {'name': 'decapoda-research/llama-7b-hf',\n",
       "  'address': '0.0.0.0:8083',\n",
       "  'owner': 'patrick',\n",
       "  'is_quantized': False}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current models and pick the first one\n",
    "TGI_CENTRAL_ADDRESS=\"localhost:8765\"\n",
    "#central_url = f\"http://{TGI_CENTRAL_ADDRESS}\"\n",
    "models = Client.list_from_central(central_url=f\"http://{TGI_CENTRAL_ADDRESS}\")\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302215e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upstage/Llama-2-70b-instruct-v2 0.0.0.0:8082\n"
     ]
    }
   ],
   "source": [
    "model_name, model_addr = models[0][\"name\"], models[0][\"address\"]\n",
    "print (model_name, model_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53cea6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_addr)\n\u001b[0;32m----> 2\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (score)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Client' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "client = Client(\"http://\" + model_addr)\n",
    "score = client.score()\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d09abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prompt, timeout, max_new_tokens):\n",
    "    print(f\"Using model {model_name} at {model_addr}\")\n",
    "    print (prompt)\n",
    "    client = Client(\"http://\" + model_addr)\n",
    "    client.timeout = timeout\n",
    "    return client.generate(prompt, do_sample=False, max_new_tokens=1, decoder_input_details=True).details.prefill[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ed5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Good morning, Mr Smith. I appreciate your help yesterday. Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[InputToken(id=835, text='###', logprob=-13.34375),\n",
       " InputToken(id=4911, text='User', logprob=-8.921875),\n",
       " InputToken(id=29901, text=':', logprob=-0.24682617),\n",
       " InputToken(id=13, text='<0x0A>', logprob=-1.1835938),\n",
       " InputToken(id=4300, text='Trans', logprob=-7.9765625),\n",
       " InputToken(id=9632, text='late', logprob=-0.32104492),\n",
       " InputToken(id=515, text='from', logprob=-3.2382812),\n",
       " InputToken(id=4223, text='English', logprob=-0.8149414),\n",
       " InputToken(id=304, text='to', logprob=-0.03125),\n",
       " InputToken(id=10369, text='Japanese', logprob=-3.5820312),\n",
       " InputToken(id=29901, text=':', logprob=-0.38427734),\n",
       " InputToken(id=13, text='<0x0A>', logprob=-1.2236328),\n",
       " InputToken(id=18420, text='Good', logprob=-7.8242188),\n",
       " InputToken(id=7250, text='morning', logprob=-0.4802246),\n",
       " InputToken(id=29892, text=',', logprob=-0.7192383),\n",
       " InputToken(id=3237, text='Mr', logprob=-3.0429688),\n",
       " InputToken(id=7075, text='Smith', logprob=-5.5820312),\n",
       " InputToken(id=29889, text='.', logprob=-0.14001465),\n",
       " InputToken(id=306, text='I', logprob=-2.6953125),\n",
       " InputToken(id=11188, text='appreciate', logprob=-7.2265625),\n",
       " InputToken(id=596, text='your', logprob=-0.40551758),\n",
       " InputToken(id=1371, text='help', logprob=-1.7207031),\n",
       " InputToken(id=22600, text='yesterday', logprob=-1.4091797),\n",
       " InputToken(id=29889, text='.', logprob=-0.049041748),\n",
       " InputToken(id=1815, text='Can', logprob=-5.3476562),\n",
       " InputToken(id=366, text='you', logprob=-1.3066406),\n",
       " InputToken(id=1371, text='help', logprob=-1.7949219),\n",
       " InputToken(id=592, text='me', logprob=-0.010765076),\n",
       " InputToken(id=411, text='with', logprob=-0.8955078),\n",
       " InputToken(id=445, text='this', logprob=-2.9941406),\n",
       " InputToken(id=12827, text='assignment', logprob=-5.4570312),\n",
       " InputToken(id=29973, text='?', logprob=-1.3105469),\n",
       " InputToken(id=13, text='<0x0A>', logprob=-0.38232422),\n",
       " InputToken(id=13, text='<0x0A>', logprob=-1.1835938),\n",
       " InputToken(id=2277, text='##', logprob=-5.25),\n",
       " InputToken(id=29937, text='#', logprob=-0.19494629),\n",
       " InputToken(id=4007, text='Ass', logprob=-8.46875),\n",
       " InputToken(id=22137, text='istant', logprob=-0.29174805),\n",
       " InputToken(id=29901, text=':', logprob=-0.31420898),\n",
       " InputToken(id=13, text='<0x0A>', logprob=-0.00035119057)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score(prompt, timeout, max_new_tokens):\n",
    "    print(f\"Using model {model_name} at {model_addr}\")\n",
    "    print (prompt)\n",
    "    client = Client(\"http://\" + model_addr)\n",
    "    client.timeout = timeout\n",
    "    return client.generate(prompt, do_sample=False, max_new_tokens=1, decoder_input_details=True).details.prefill[1:]\n",
    "\n",
    "prompt15 = '''### User:\n",
    "Translate from English to Japanese:\n",
    "Good morning, Mr Smith. I appreciate your help yesterday. Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "score(prompt15, 50, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
