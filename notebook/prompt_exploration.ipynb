{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aec7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5af381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ~/.bashrc: executed by bash(1) for non-login shells.\n",
      "# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)\n",
      "# for examples\n",
      "\n",
      "# If not running interactively, don't do anything\n",
      "case $- in\n",
      "    *i*) ;;\n",
      "      *) return;;\n",
      "esac\n",
      "\n",
      "# don't put duplicate lines or lines starting with space in the history.\n",
      "# See bash(1) for more options\n",
      "HISTCONTROL=ignoreboth\n",
      "\n",
      "# append to the history file, don't overwrite it\n",
      "shopt -s histappend\n",
      "\n",
      "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
      "HISTSIZE=1000\n",
      "HISTFILESIZE=2000\n",
      "\n",
      "# check the window size after each command and, if necessary,\n",
      "# update the values of LINES and COLUMNS.\n",
      "shopt -s checkwinsize\n",
      "\n",
      "# If set, the pattern \"**\" used in a pathname expansion context will\n",
      "# match all files and zero or more directories and subdirectories.\n",
      "#shopt -s globstar\n",
      "\n",
      "# make less more friendly for non-text input files, see lesspipe(1)\n",
      "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
      "\n",
      "# set variable identifying the chroot you work in (used in the prompt below)\n",
      "if [ -z \"${debian_chroot:-}\" ] && [ -r /etc/debian_chroot ]; then\n",
      "    debian_chroot=$(cat /etc/debian_chroot)\n",
      "fi\n",
      "\n",
      "# set a fancy prompt (non-color, unless we know we \"want\" color)\n",
      "case \"$TERM\" in\n",
      "    xterm-color|*-256color) color_prompt=yes;;\n",
      "esac\n",
      "\n",
      "# uncomment for a colored prompt, if the terminal has the capability; turned\n",
      "# off by default to not distract the user: the focus in a terminal window\n",
      "# should be on the output of commands, not on the prompt\n",
      "#force_color_prompt=yes\n",
      "\n",
      "if [ -n \"$force_color_prompt\" ]; then\n",
      "    if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then\n",
      "\t# We have color support; assume it's compliant with Ecma-48\n",
      "\t# (ISO/IEC-6429). (Lack of such support is extremely rare, and such\n",
      "\t# a case would tend to support setf rather than setaf.)\n",
      "\tcolor_prompt=yes\n",
      "    else\n",
      "\tcolor_prompt=\n",
      "    fi\n",
      "fi\n",
      "\n",
      "if [ \"$color_prompt\" = yes ]; then\n",
      "    PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
      "else\n",
      "    PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ '\n",
      "fi\n",
      "unset color_prompt force_color_prompt\n",
      "\n",
      "# If this is an xterm set the title to user@host:dir\n",
      "case \"$TERM\" in\n",
      "xterm*|rxvt*)\n",
      "    PS1=\"\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\"\n",
      "    ;;\n",
      "*)\n",
      "    ;;\n",
      "esac\n",
      "\n",
      "# enable color support of ls and also add handy aliases\n",
      "if [ -x /usr/bin/dircolors ]; then\n",
      "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
      "    alias ls='ls --color=auto'\n",
      "    #alias dir='dir --color=auto'\n",
      "    #alias vdir='vdir --color=auto'\n",
      "\n",
      "    alias grep='grep --color=auto'\n",
      "    alias fgrep='fgrep --color=auto'\n",
      "    alias egrep='egrep --color=auto'\n",
      "fi\n",
      "\n",
      "# colored GCC warnings and errors\n",
      "#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'\n",
      "\n",
      "# some more ls aliases\n",
      "alias ll='ls -alF'\n",
      "alias la='ls -A'\n",
      "alias l='ls -CF'\n",
      "\n",
      "# Add an \"alert\" alias for long running commands.  Use like so:\n",
      "#   sleep 10; alert\n",
      "alias alert='notify-send --urgency=low -i \"$([ $? = 0 ] && echo terminal || echo error)\" \"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;&|]\\s*alert$//'\\'')\"'\n",
      "\n",
      "# Alias definitions.\n",
      "# You may want to put all your additions into a separate file like\n",
      "# ~/.bash_aliases, instead of adding them here directly.\n",
      "# See /usr/share/doc/bash-doc/examples in the bash-doc package.\n",
      "\n",
      "if [ -f ~/.bash_aliases ]; then\n",
      "    . ~/.bash_aliases\n",
      "fi\n",
      "\n",
      "# enable programmable completion features (you don't need to enable\n",
      "# this, if it's already enabled in /etc/bash.bashrc and /etc/profile\n",
      "# sources /etc/bash.bashrc).\n",
      "if ! shopt -oq posix; then\n",
      "  if [ -f /usr/share/bash-completion/bash_completion ]; then\n",
      "    . /usr/share/bash-completion/bash_completion\n",
      "  elif [ -f /etc/bash_completion ]; then\n",
      "    . /etc/bash_completion\n",
      "  fi\n",
      "fi\n",
      "\n",
      "# >>> conda initialize >>>\n",
      "# !! Contents within this block are managed by 'conda init' !!\n",
      "__conda_setup=\"$('/home/sumire/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
      "if [ $? -eq 0 ]; then\n",
      "    eval \"$__conda_setup\"\n",
      "else\n",
      "    if [ -f \"/home/sumire/miniconda3/etc/profile.d/conda.sh\" ]; then\n",
      "        . \"/home/sumire/miniconda3/etc/profile.d/conda.sh\"\n",
      "    else\n",
      "        export PATH=\"/home/sumire/miniconda3/bin:$PATH\"\n",
      "    fi\n",
      "fi\n",
      "unset __conda_setup\n",
      "# <<< conda initialize <<<\n",
      "\n",
      "export TGI_CENTRAL_ADDRESS=localhost:8765\n",
      "\n",
      "export PATH=\"/home/sumire/miniconda3/envs/muda_env/lib/python3.9/site-packages/torch/cuda:$PATH\"\n"
     ]
    }
   ],
   "source": [
    "!cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417bcf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:8765\n"
     ]
    }
   ],
   "source": [
    "TGI_CENTRAL_ADDRESS=os.environ.get('TGI_CENTRAL_ADDRESS')\n",
    "print(TGI_CENTRAL_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfe5233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:8765\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8765): Max retries exceeded with url: /list_models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2b2161bfd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/http/client.py:1230\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/http/client.py:1276\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1276\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/http/client.py:1225\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1225\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/http/client.py:1004\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1004\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1006\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m \n\u001b[1;32m   1008\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/http/client.py:944\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 944\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    945\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f2b2161bfd0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8765): Max retries exceeded with url: /list_models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2b2161bfd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook/prompt_exploration.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook/prompt_exploration.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m TGI_CENTRAL_ADDRESS\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mTGI_CENTRAL_ADDRESS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook/prompt_exploration.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(TGI_CENTRAL_ADDRESS)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook/prompt_exploration.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m models \u001b[39m=\u001b[39m Client\u001b[39m.\u001b[39;49mlist_from_central(central_url\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttp://\u001b[39;49m\u001b[39m{\u001b[39;49;00mTGI_CENTRAL_ADDRESS\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/sumire/thesis/LLM_Contextual_Prompt_MT/notebook/prompt_exploration.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m models\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/text_generation/client.py:63\u001b[0m, in \u001b[0;36mClient.list_from_central\u001b[0;34m(cls, central_url)\u001b[0m\n\u001b[1;32m     60\u001b[0m     central_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mTGI_CENTRAL_ADDRESS\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[39m# query from /models endpoint\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcentral_url\u001b[39m}\u001b[39;49;00m\u001b[39m/list_models\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m payload \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_mt3/lib/python3.8/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8765): Max retries exceeded with url: /list_models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2b2161bfd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "TGI_CENTRAL_ADDRESS=os.environ.get('TGI_CENTRAL_ADDRESS')\n",
    "#print(TGI_CENTRAL_ADDRESS)\n",
    "models = Client.list_from_central(central_url=f\"http://{TGI_CENTRAL_ADDRESS}\")\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a5edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upstage/Llama-2-70b-instruct-v2 0.0.0.0:8082\n"
     ]
    }
   ],
   "source": [
    "model_name, model_addr = models[0][\"name\"], models[0][\"address\"]\n",
    "print (model_name, model_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d55515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, timeout, max_new_tokens):\n",
    "    print(f\"Using model {model_name} at {model_addr}\")\n",
    "    print (prompt)\n",
    "    client = Client(\"http://\" + model_addr)\n",
    "    client.timeout = timeout\n",
    "    return client.generate(prompt, max_new_tokens=max_new_tokens).generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5a943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Answer the formality level below in one word.\n",
      "Good morning, Mr Smith. I appreciate your help yesterday. Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Formal'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_size = 64, 2-1\n",
    "prompt15 = '''### User:\n",
    "Answer the formality level below in one word.\n",
    "Good morning, Mr Smith. I appreciate your help yesterday. Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4caa7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Answer the formality level below in one word.\n",
      "Hey David, what's up, Grab some pizza. \n",
      "By the way, I missed yesterday's lecture.\n",
      "Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Informal'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt15 =\"\"\"### User:\n",
    "Answer the formality level below in one word.\n",
    "Hey David, what's up, Grab some pizza. \n",
    "By the way, I missed yesterday's lecture.\n",
    "Can you help me with this assignment?\\n\\n### Assistant:\\n\"\"\"\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc150279",
   "metadata": {},
   "source": [
    "The reference we want to translate after postprocess: \"今や真っ黒な出で立ちで とても落ち込んでいます 皆さんは素敵だと 思われるかもしれませんが 心は死んでいるんです 選択肢がなかったのですから\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8b90b",
   "metadata": {},
   "source": [
    "For 2-1 already, max_new_tokens = 64 is not enough\n",
    "For this example 128 is a good size with 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e657d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_size = 64, 2-1\n",
    "prompt0='''Translate English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>'''\n",
    "\n",
    "generate_text(prompt0, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cb686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです あなたたちは私たちが可愛いと思います でも私たちの中は死んでいます 選択肢がありませんから\\nI'm Jessi, and this is\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_size = 128\n",
    "generate_text(prompt0, 20, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52488fdc",
   "metadata": {},
   "source": [
    "changing the max_new_token will worsen the translation? \n",
    "-> no, so after this we do 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d47d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです あなたたちは私たちが可愛いと思います でも私たちの中は死んでいます 選択肢がありませんから\\nI'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed.\\n私はジェシー\\u3000これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\\n<b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired.\\n<b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\\nGood afternoon, everybody. <b> I've got something to show you.\\n皆さんこんにちは <b> 今日はあるものをお持ちしました\\nAnd we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\\n私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです あなたたちは私たちが可愛いと思います でも私たちの中は死んでいます 選択肢がありませんから\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_size = 1024\n",
    "generate_text(prompt0, 100, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8976608",
   "metadata": {},
   "source": [
    "With Bad prompt VS without -> actually better because the output generates break token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa687b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      "<b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"そして私たちは私たちは私たちは 皆さんを完全に疎外させるまで 押し続けます <b> 私たちは全員黒ずくめになり 非常に憂鬱になります あなたたちは私たちが可愛いと思います しかし私たちの中は死んでいます なぜなら私たちに選択肢はありませんからです\\n<b> I'm going to show you a few things  that I've got inside my suitcase. => <b> 私はスーツケースの中に入っている いくつかの物をお見せします\\n<b> I'm going to show you a few things  that I've got inside my suitcase. => <b> 私はスーツ\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with repeated bad reference in shot\n",
    "prompt1='''Translate English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    "<b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>'''\n",
    "\n",
    "generate_text(prompt1, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac8f099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです あなたたちは私たちが可愛いと思います でも私たちの中は死んでいます 選択肢がありませんから\\nI'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed.\\n私はジェシー\\u3000これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\\n<b> I\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without repeated bad reference in shot\n",
    "\n",
    "generate_text(prompt0, 50, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c8ada",
   "metadata": {},
   "source": [
    "With Context or Without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547de062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "And we push and we push and we push  until we completely alienate all of you. <b>We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは押し続けます 皆さんを孤立させます <b> 私たちは全員黒を着ます とても悲しいです あなたたちは私たちが可愛いと思います でも私たちの中は死んでいます 選択肢がありませんから\\nI'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed.\\n私はジェシー\\u3000これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\\n<b> I\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-1\n",
    "generate_text(prompt0, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f2fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'一方は革新です 建築家は常に新しい技術、新しいタイプ、今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1\n",
    "prompt2 = '''Translate English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>'''\n",
    "\n",
    "generate_text(prompt2, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea1c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'一方は革新です 建築家は常に新しい技術、新しいタイプ、今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます 今日の生活に対する'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1 we tested\n",
    "prompt_tested = '''Translate English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>'''\n",
    "\n",
    "generate_text(prompt_tested, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7242fe7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate English to Japanese:\n",
      "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "I've got something to show you. => 今日はあるものをお持ちしました\n",
      "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは全身黒ずくめで とても悲しいです あなたは私たちがかわいいと思います しかし私たちの中は死んでいます なぜなら私たちに選択肢はありませんから\\nI'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私は公開の告白をします それは私がファッションに夢中であることです\\nI'm outfit-obsessed. => 私はファッションに夢中です\\nI'm going to make a very public confession,  and that is,  I'm outfit-obsessed.\\n私は公開の告白をします それは私がファッションに夢中であることです\\nI'm out\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without context 1-1 \n",
    "prompt3 = '''Translate English to Japanese:\n",
    "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "I've got something to show you. => 今日はあるものをお持ちしました\n",
    "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>'''\n",
    "\n",
    "generate_text(prompt3, 50, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43ad2b",
   "metadata": {},
   "source": [
    "different example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d4df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following text from English to German:\n",
      "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
      "natural language processing community due to its potential for various information access applications.\n",
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate the following text from English to German:\n",
      "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
      "natural language processing community due to its potential for various information access applications.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</b>\\nEinzel-Dokument-Zusammenfassung ist die Aufgabe, eine kürzere Version eines Dokuments automatisch zu erstellen, wobei die wichtigsten Informationen beibehalten werden. Die Aufgabe hat in der Gemeinschaft der natürlichen Sprachverarbeitung viel Aufmerksamkeit erhalten, aufgrund ihres potenziellen Nutzens für verschiedene Informationszugangsanwendungen.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero shot, 2-1, => doesn't translate to target language\n",
    "prompt4='''Translate the following text from English to German:\n",
    "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
    "natural language processing community due to its potential for various information access applications.'''\n",
    "print (prompt4)\n",
    "\n",
    "generate_text(prompt4, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0f193e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate the following text from English to German:\n",
      "The task has received much attention in the natural language processing community due to its potential for various information access applications.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDie Aufgabe hat in der Gemeinschaft der natürlichen Sprachverarbeitung viel Aufmerksamkeit erhalten, aufgrund ihres Potenzials für verschiedene Informationszugangsanwendungen.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero shot, 1-1 => doesn't translate to target language \n",
    "prompt4='''Translate the following text from English to German:\n",
    "The task has received much attention in the natural language processing community due to its potential for various information access applications.'''\n",
    "\n",
    "generate_text(prompt4, 50, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddeb3e",
   "metadata": {},
   "source": [
    "Putting the pretrained specific prompt, User/Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca4f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
      "natural language processing community due to its potential for various information access applications.\n",
      "\n",
      "### Assistant:\n",
      "\n",
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
      "natural language processing community due to its potential for various information access applications.\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'単一文書の要約は、文書の最も重要な情報を保持しながら、自動的に文書の短いバージョンを生成するタスクです。<b> このタスクは、様々な情報アクセスアプリケーションの可能性があるため、自然言語処理コミュニティで多くの注目を集めています。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Not understanding <break> token ==> <b> is better thant <break>\n",
    "prompt5='''### User:\\nTranslate from English to Japanese:\n",
    "Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. <b> The task has received much attention in the\n",
    "natural language processing community due to its potential for various information access applications.\\n\\n### Assistant:\\n'''\n",
    "\n",
    "print (prompt5)\n",
    "\n",
    "generate_text(prompt5, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e5e907a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "I've got something to show you. => 今日はあるものをお持ちしました\n",
      "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"私たちは全身黒ずくめで とても悲しいです あなたは私たちがかわいいと思います しかし、私たちの中は死んでいます なぜなら私たちに選択肢はありませんから\\nI'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私は公開で告白します それは、私はファッションに夢中です\\nI'm outfit-obsessed. => 私はファッションに夢中です\\nI'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私は公開で告白します それは、私はファッションに夢中です\\nI'm outfit-obsessed. => �\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good but only for 1-1, not good for contextual model\n",
    "# with \"User on the top, 1-1\" ==> Translating OK, don't need postprocess\n",
    "prompt6='''### User:\\nTranslate from English to Japanese:\n",
    "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "I've got something to show you. => 今日はあるものをお持ちしました\n",
    "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>'''\n",
    "\n",
    "generate_text(prompt6, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4833130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "I've got something to show you. => 今日はあるものをお持ちしました\n",
      "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. => \n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'でも、今日は中に入っているものをお見せします前に、皆さんの前である告白をします。私は洋服に夢中です。\\nリビア革命が勃発してからおよそ２年が経ちました。大規模な騒乱の波、チュニジアとエジプトでの革命の流れを受け、リビア革命が勃発してからおよそ２年が経ちました。大規模な騒乱の波、チュニジアとエジプトでの革命の流れを受け。\\n数ヶ月前に雑誌の記事を執筆していたときに発見したことをお話しします。\\n今日はあるものをお持ち'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-1 with \"Assistant in the end\" ==> Only Repeating the shots, not translating the last source\n",
    "prompt7='''### User:\\nTranslate from English to Japanese:\n",
    "But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    "I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. => 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "I've got something to show you. => 今日はあるものをお持ちしました\n",
    "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. => \\n\\n### Assistant:\\n'''\n",
    "\n",
    "\n",
    "generate_text(prompt7, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9deba257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"一方は革新です 建築家たちは常に新しい技術、新しいタイポロジー、今日の生活に対する新しい解決策を求めています そして私たちは押し続けます 押し続けます 押し続けます あなたたちが私たちを完全に疎外させるまで <b> 私たちは全員黒を着ています 悲しいです あなたたちは私たちが可愛いと思います 私たちの中は死んでいます なぜなら私たちに選択肢はありませんからです\\nI'm going to tell you a story about a man named John. <b> John was a very successful businessman.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1 with User on the top ==> it is not translating , rather generating new contents, without \"Assistant\"\n",
    "prompt8 = '''### User:\\nTranslate from English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice. =>'''\n",
    "\n",
    "\n",
    "generate_text(prompt8, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fde5e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\n",
      "\n",
      "### Assistant:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'片方には革新があり、建築家たちは常に新しい技術、新しいタイポロジー、今日の生活に対する新しい解決策を求めています。そして、私たちは押し続け、押し続け、押し続けます。そして、私たちは全員を孤立させてしまいます。<b> 私たちは全員黒を着て、非常に憂鬱になります。あなたたちは私たちが可愛いと思いますが、私たちの中は死んでいます。私たちには選択肢がありません。'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Good \n",
    "# 3-1, User/assistant, ZERO shots ==> Translating even without shots and generate break token ==> good for contextual model with short inputs\n",
    "prompt9 = '''### User:\\nTranslate from English to Japanese:\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\\n\\n### Assistant:\\n '''\n",
    "\n",
    "generate_text(prompt9, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e35e9ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate the last sentence after <b> from English to Japanese:\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\n",
      "\n",
      "### Assistant:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'一方は革新で、建築家たちは常に新しい技術、新しいタイポロジー、今日の生活に対する新しい解決策を求めています。そして、私たちは押し続け、押し続け、押し続け、あなたたち全員を完全に疎外させます。<b> 私たちは全員黒を着て、非常に憂鬱になります、あなたたちは私たちが可愛いと思いますが、私たちの中は死んでいます、なぜなら私たちに選択肢はありませんから。'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The instruction like \"Translate the last sentence after <b> \" doesn't work\n",
    "prompt10 = '''### User:\\nTranslate the last sentence after <b> from English to Japanese:\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you. <b> We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\\n\\n### Assistant:\\n '''\n",
    "\n",
    "generate_text(prompt10, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea7d127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Given context:\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you.\n",
      "Translate from English to Japanese:\n",
      "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\n",
      "\n",
      "### Assistant:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'黒い服を着て、とても悲しくなります。あなたたちは私たちが可愛いと思いますが、私たちの中は死んでいます。なぜなら、私たちに選択肢はありませんからです。'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Good (?)\n",
    "# 3-1, User/assistant Zero shots with contexts with \"Given Contexts\" ### but another example below needs postprocess with \\n \n",
    "prompt12 = '''### User:\n",
    "Given context:\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you.\n",
    "Translate from English to Japanese:\n",
    "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\\n\\n### Assistant:\\n '''\n",
    "\n",
    "generate_text(prompt12, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e968ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Given context, translate only the source sentence to Japanese:\n",
      "Context:\n",
      "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you.\n",
      "Source sentence:\n",
      "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\n",
      "\n",
      "### Assistant:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'私たちは全身黒を着て、とても悲しいです。あなたたちは私たちがかわいいと思いますが、私たちの中は死んでいます。私たちには選択肢がありません。'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good\n",
    "# 3-1, User/assistant Zero shots with contexts with \"Gien context translate only the source sentence\" \n",
    "prompt12 = '''### User:\n",
    "Given context, translate only the source sentence to Japanese:\n",
    "Context:\n",
    "On the one side is innovation,  and architects are constantly pushing, pushing for new technologies,  new typologies, new solutions for the way that we live today.And we push and we push and we push  until we completely alienate all of you.\n",
    "Source sentence:\n",
    "We wear all black, we get very depressed,  you think we're adorable,  we're dead inside because we've got no choice.\\n\\n### Assistant:\\n '''\n",
    "\n",
    "generate_text(prompt12, 50, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6a7f8",
   "metadata": {},
   "source": [
    "Try obvious contextual example (not in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e131ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate from English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "Good morning Mr. Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"おはようございます スミスさん 昨日のお手伝いありがとうございます <b> この宿題のお手伝いをしてくださいませんか？\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが それについてはお手伝いできません <b> 私はそのテーマについては知りません\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが それについてはお手伝いできません <b> 私はそのテーマについては知りません\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが \""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few shots ==> works good for poli#te expression\n",
    "prompt8 = '''Translate from English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "Good morning Mr. Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>'''\n",
    "generate_text(prompt8, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cb1a62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "Good morning Mr. Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"おはようございます スミスさん 昨日のお手伝いありがとうございました <b> この宿題のお手伝いをしてくださいませんか？\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが それはできません <b> 私はそのテーマについては知りません\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが それはできません <b> 私はそのテーマについては知りません\\nI'm sorry, but I can't help you with that. <b> I'm not familiar with the subject. => すみませんが それはできません <b> 私\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Good \n",
    "# Without Assistant but with User + few shots ==> works good for polite expression\n",
    "prompt8 = '''### User:\\nTranslate from English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "Good morning Mr. Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>'''\n",
    "generate_text(prompt8, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fefe43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
      "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
      " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
      "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
      "Hey David, what's up, Grab some pizza. By the way, I missed yesterday's lecture. <b> Can you help me with this assignment? =>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"おはようデビッド 何か起こったの？ ピザを取ってきて ちょっと、昨日の講義を見逃しました <b> この宿題を手伝ってくれませんか？\\nI'm sorry, I'm not a native speaker, but I'll try my best to help you. <b> I'm not sure if I can do it perfectly, but I'll do my best. => すみません、私はネイティブスピーカーではありませんが 私の力の限りを尽くします <b> 完璧にできるかどうかはわかりませんが 私のベストを尽くします\\nI'm sorry, I'm not a native speaker, but I'll try my best to help you. <b> I'm not sure if I can do it perfectly, but I'll do my best.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good (same above)\n",
    "# Without Assistant but with few shots => works ok for little bit casual (not coompletely casual)\n",
    "prompt8 = '''### User:\\nTranslate from English to Japanese:\n",
    "I'm Jessi, and this is my suitcase. <b> But before I show you what I've got inside,  I'm going to make a very public confession,  and that is,  I'm outfit-obsessed. => 私はジェシー　これは私のスーツケースです <b> 中に入っている物をお見せする前に 皆さんの前で ある告白をします 私は 洋服に夢中です\n",
    "I have never, ever forgotten the words of my grandmother  who died in her exile:  \"Son, resist Gaddafi. Fight him.But don't you ever turn  into a Gaddafi-like revolutionary.\" <b> Almost two years have passed  since the Libyan Revolution broke out,  inspired by the waves of mass mobilization  in both the Tunisian and the Egyptian revolutions. => 私は一度たりとも忘れたことはありません 亡命中に亡くなった祖母の言葉を 「息子よ カダフィに抵抗し 戦いなさいでもそれは決して カダフィの様な革命であってはなりません」 <b> リビア革命が勃発してから およそ２年が経ちました リビア革命が勃発してから およそ２年が経ちました 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け 大規模な騒乱の波 チュニジアとエジプトでの革命の流れを受け\n",
    " <b> I'd like to share with you a discovery that I made a few months ago  while writing an article for Italian Wired. =>  <b> 数ヶ月前に雑誌の記事を執筆していたときに 発見したことをお話しします\n",
    "Good afternoon, everybody. <b> I've got something to show you. => 皆さんこんにちは <b> 今日はあるものをお持ちしました\n",
    "Hey David, what's up, Grab some pizza. By the way, I missed yesterday's lecture. <b> Can you help me with this assignment? =>'''\n",
    "generate_text(prompt8, 50, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b1fc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Given context:\n",
      "Good morning Mr. Smith.\n",
      "I appreciate your help yesterday. \n",
      "Translate from English to Japanese below:\n",
      "Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'おはようございます、スミスさん。\\n昨日のお助けありがとうございました。\\nこの宿題を手伝ってください。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### need postprocess\n",
    "# 3-1, User/assistant Zero shots with contexts with \"Given Contexts\"\n",
    "prompt13 = '''### User:\n",
    "Given context:\n",
    "Good morning Mr. Smith.\n",
    "I appreciate your help yesterday. \n",
    "Translate from English to Japanese:\n",
    "Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt13, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bdab5344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Given context: \n",
      "Hey David, what's up, Grab some pizza. \n",
      "By the way, I missed yesterday's lecture. \n",
      "Translate from English to Japanese:\n",
      "Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'はい、デビッド、何か起こっていますか？ピザを取ってください。 それにしても、昨日の講義を見逃しました。 この宿題を手伝ってくれますか？'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cannot capture context\n",
    "# 3-1, User/assistant ZERO shots ==> Translating even without shots \n",
    "prompt14 = '''### User:\n",
    "Given context: \n",
    "Hey David, what's up, Grab some pizza. \n",
    "By the way, I missed yesterday's lecture. \n",
    "Translate from English to Japanese:\n",
    "Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt14, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c319b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Given context, translate the source sentence to Japanese sentence in an appropriate style:\n",
      "Context:\n",
      "Hey David, what's up, Grab some pizza. \n",
      "By the way, I missed yesterday's lecture.\n",
      "Source sentence:\n",
      "Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'おはよう、デビッド。ピザを取ってきて。ちなみに、昨日の講義を見逃しました。\\nこの宿題を手伝ってくれますか？'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate context and source sentence and \"in an appropriate style\" doesn't work to read context\n",
    "prompt14 = '''### User:\n",
    "Given context, translate the source sentence to Japanese sentence in an appropriate style:\n",
    "Context:\n",
    "Hey David, what's up, Grab some pizza. \n",
    "By the way, I missed yesterday's lecture.\n",
    "Source sentence:\n",
    "Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt14, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba9c31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Hey David, what's up, Grab some pizza. By the way, I missed yesterday's lecture. <b> Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'デイビッド、こんにちは、ピザを取ってくれない？それにしても、昨日の講義を見逃してしまったんだ。<b> この宿題を手伝ってくれる？'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Good \n",
    "# zero-shot + User/Assistant + <b>\n",
    "# Genearate context + source with <b> makes the appropriate formality\n",
    "prompt15 = '''### User:\n",
    "Translate from English to Japanese:\n",
    "Hey David, what's up, Grab some pizza. By the way, I missed yesterday's lecture. <b> Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5189814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment?\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'おはようございます、スミスさん。昨日のお助けありがとうございました。<b> この宿題を手伝ってくださいませんか？'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################NEXT PROMPT ########################################################\n",
    "### Good \n",
    "# Genearate context + source with <b> makes the appropriate formality\n",
    "prompt15 = '''### User:\n",
    "Translate from English to Japanese:\n",
    "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment?\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c528b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'おはようございます、スミスさん。昨日のお助けありがとうございました。<b> この宿題を手伝ってくださいませんか？'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding => in the end before Assistant? Fine, doesn't matter\n",
    "### Good \n",
    "# Genearate context + source with <b> makes the appropriate formality\n",
    "prompt15 = '''### User:\n",
    "Translate from English to Japanese:\n",
    "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment? =>\\n\\n### Assistant:\\n'''\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fb4e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "Translate from English to Japanese:\n",
      "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</b> I need to translate a sentence from English to Japanese. The sentence is: \"Good morning, Mr Smith. I appreciate your help yesterday. Can you help me with this assignment?\"\\nおはようございます、スミスさん。昨日のお助けありがとうございます。この宿題のためにお手伝いしてください。「おはようございます、スミスさん。昨日のお助けありがとうございます。この宿題のためにお手伝いしてください。」'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We seems really need User/Assistant\n",
    "# Genearate context + source with <b> makes the appropriate formality\n",
    "prompt15 = '''Translate from English to Japanese:\n",
    "Good morning, Mr Smith. I appreciate your help yesterday. <b> Can you help me with this assignment?'''\n",
    "\n",
    "generate_text(prompt15, 50, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64ef7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model upstage/Llama-2-70b-instruct-v2 at 0.0.0.0:8082\n",
      "### User:\n",
      "Translate from English to Japanese:\n",
      "GAnd so, several years ago,  I undertook a program to try to understand  the fundamental physical mechanisms underlying intelligence. <b> Let's take a step back. =>\n",
      "\n",
      "### Assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt15 = '''### User:\n",
    "Translate from English to Japanese:\n",
    "And so, several years ago,  I undertook a program to try to understand  the fundamental physical mechanisms underlying intelligence. <b> Let's take a step back. =>\\n\\n### Assistant:\\n'''\n",
    "\n",
    "text = generate_text(prompt15, 50, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b420d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "それで、数年前、私は知性の基本的な物理的メカニズムを理解しようとするプログラムを始めました。<b> 少し後退しましょう。\n",
      "<b> founded\n",
      "[' 少し後退しましょう。']\n"
     ]
    }
   ],
   "source": [
    "print (text)\n",
    "break_token = \" <b>\"\n",
    "\n",
    "if break_token in text or \"<b>\" in text:\n",
    "    print (\"<b> founded\")\n",
    "    preds = [text.split(\"<b>\")[1]]\n",
    "    print (preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abd917",
   "metadata": {},
   "source": [
    "- max_new_token = 256 \n",
    "- timeout = 50\n",
    "- this example doesn't improve 皆さん translation after context, rather あなた　from あなたたち\n",
    "- When we use \"Assistant\" in the end of the prompt, we shouldn't put shots (shots + User/Assistant is bad)\n",
    "- When we use \"User\" only, we can put shots (shots + User is good, but shots + User + context is bad)\n",
    "- \"b\" is better thant \"break\", so they output the token correctl in output\n",
    "- If zero shot we must use Assistant/User\n",
    "- Context + shot + User/Assistant is bad \n",
    "- Context + user/Assistant is good \n",
    "\n",
    "==> Question: Shots + Context VS User/Assistant Context \n",
    "==>Our question is not to find the best few shots example (and the random reference is bad often the time), so zero-shot contextual model would be more interesting \n",
    "\n",
    "Next prompt1:\n",
    "- zero shot\n",
    "- With User/Assistant\n",
    "- prompt 3 style with \"b\" token\n",
    "- timeout 50 or 100\n",
    "- max_new_token 256 or 512\n",
    "    \n",
    "Next prompt2 (?):\n",
    "- fewshot\n",
    "- With \"User\" but without \"Assistant\"\n",
    "- max token 256 or more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
