{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18234275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a561639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2aff7b021f107e53\n",
      "Found cached dataset json (/home/sumire/.cache/huggingface/datasets/json/default-2aff7b021f107e53/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c93339a443416f95b8ef60cf40138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['talk_id', 'doc'],\n",
       "        num_rows: 93\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_lang = \"de\"\n",
    "file_path = \"/home/sumire/thesis/LLM_Contextual_Prompt_MT/data/iwslt_hf/\"\n",
    "\n",
    "data_files = { \"test\": f\"{file_path}ted_en-{tgt_lang}\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaed7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label ={0: \"anger\", 1: \"fear\", 2: \"joy\", 3: \"sadness\"}\n",
    "label2id = {\"anger\" : 0, \"fear\" : 1, \"joy\": 2, \"sadness\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6808aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"MilaNLProc/xlm-emo-t\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"MilaNLProc/xlm-emo-t\", num_labels=4, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c822f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    inputs = [sent for doc in data[\"doc\"] for sent in doc[\"en\"]][:50]\n",
    "    #inputs = [kshot + sent + ' = ' for doc in data[\"doc\"] for sent in doc[\"en\"] ][:50]\n",
    "    return tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a4b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f4f10344fe4fa3b9549154a170285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names,)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106b021c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sumire/thesis/LLM_Contextual_Prompt_MT/data/iwslt_hf/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m lang_data_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tgt_lang \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlang_list\u001b[49m:\n\u001b[1;32m      7\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mted_en-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      8\u001b[0m     dataset[tgt_lang] \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39mdata_files)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lang_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Accuracies for each tgt_language \n",
    "tgt_lang_list = [\"ja\", \"de\", \"fr\", \"zh\", \"ar\", \"ko\"]\n",
    "file_path = \"/home/sumire/thesis/LLM_Contextual_Prompt_MT/data/iwslt_hf/\"\n",
    "\n",
    "lang_data_dict = {}\n",
    "for tgt_lang in lang_list:\n",
    "    data_files = { \"test\": f\"{file_path}ted_en-{tgt_lang}\"}\n",
    "    dataset[tgt_lang] = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "    pred_list = []\n",
    "    for lang in [\"en\", tgt_lang]:\n",
    "        lang_pred_list = []\n",
    "        print (lang)\n",
    "        inputs = [sent for doc in dataset[tgt_lang][\"test\"][\"doc\"] for sent in doc[lang]]\n",
    "        tokenized_inputs = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        tokenized_inputs[\"input_ids\"]\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokenized_inputs).logits\n",
    "        for inst_logits in logits:\n",
    "            predicted_class_id = inst_logits.argmax(dim=-1).item()\n",
    "            #print (lang, \"predicted Emotion\")\n",
    "            lang_pred_list.append(model.config.id2label[predicted_class_id])\n",
    "        pred_list.append(lang_pred_list)\n",
    "\n",
    "    true_false = []\n",
    "    for en_pred, tgt_pred in zip(pred_list[0], pred_list[1]):\n",
    "        #print (en_pred, ja_pred)\n",
    "        if en_pred != tgt_pred:\n",
    "            true_false.append(False)\n",
    "        else:\n",
    "            true_false.append(True)\n",
    "\n",
    "    lang_accuracies[tgt_lang] = true_false.count(True)/len(true_false)\n",
    "lang_accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c1780a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ja': 0.44, 'ko': 0.58, 'de': 0.6, 'zh': 0.5, 'fr': 0.66, 'ar': 0.48}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Annotate the English data with the emotion label \n",
    "\n",
    "tgt_lang_list = [\"ja\", \"de\", \"fr\", \"zh\", \"ar\", \"ko\"]\n",
    "file_path = \"/home/sumire/thesis/LLM_Contextual_Prompt_MT/data/iwslt_hf/\"\n",
    "\n",
    "lang_data_dict = {}\n",
    "for tgt_lang in lang_list:\n",
    "    data_files = { \"test\": f\"{file_path}ted_en-{tgt_lang}\"}\n",
    "    dataset[tgt_lang] = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "    pred_list = []\n",
    "    for lang in [\"en\", tgt_lang]:\n",
    "        lang_pred_list = []\n",
    "        print (lang)\n",
    "        inputs = [sent for doc in dataset[tgt_lang][\"test\"][\"doc\"] for sent in doc[lang]]\n",
    "        tokenized_inputs = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        tokenized_inputs[\"input_ids\"]\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokenized_inputs).logits\n",
    "        for inst_logits in logits:\n",
    "            predicted_class_id = inst_logits.argmax(dim=-1).item()\n",
    "            #print (lang, \"predicted Emotion\")\n",
    "            lang_pred_list.append(model.config.id2label[predicted_class_id])\n",
    "        pred_list.append(lang_pred_list)\n",
    "        dataset[]\n",
    "        \n",
    "    true_false = []\n",
    "    for en_pred, tgt_pred in zip(pred_list[0], pred_list[1]):\n",
    "        #print (en_pred, ja_pred)\n",
    "        if en_pred != tgt_pred:\n",
    "            true_false.append(False)\n",
    "        else:\n",
    "            true_false.append(True)\n",
    "\n",
    "    lang_accuracies[tgt_lang] = true_false.count(True)/len(true_false)\n",
    "lang_accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c62412c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "sadness\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sadness\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "anger\n",
      "sadness\n",
      "joy\n",
      "joy\n",
      "sadness\n",
      "joy\n",
      "anger\n",
      "anger\n",
      "anger\n",
      "sadness\n",
      "sadness\n",
      "anger\n",
      "anger\n",
      "anger\n",
      "joy\n",
      "fear\n",
      "fear\n",
      "sadness\n",
      "sadness\n",
      "sadness\n",
      "sadness\n",
      "sadness\n",
      "fear\n",
      "sadness\n",
      "anger\n",
      "fear\n",
      "joy\n",
      "sadness\n",
      "fear\n",
      "joy\n",
      "sadness\n",
      "anger\n",
      "sadness\n",
      "sadness\n",
      "joy\n",
      "anger\n",
      "joy\n",
      "sadness\n",
      "anger\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenized_inputs).logits\n",
    "for sent_logits in logits:\n",
    "    predicted_class_id = sent_logits.argmax(dim=-1).item()\n",
    "    print (model.config.id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60468093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    print (\"a\")\n",
    "    preds, labels, input_ids = eval_preds\n",
    "    print (preds)\n",
    "    with open(output_dir+'/translations.txt','w', encoding='utf8') as wf:\n",
    "         for translation, ids in zip(decoded_preds, preds):\n",
    "            wf.write(translation.strip()+'\\n')\n",
    "            wf.write(str(ids)+'\\n')\n",
    "    \n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    result = {}\n",
    "    result[\"accuracy\"] = accuracy.compute(predictions=predictions, references=labels)\n",
    "    result[\"f1\"] = f1.compute(predictions=predictions, references=labels)\n",
    "    print (result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a77ad49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 50\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-7.3690170e-01,  1.9864393e+00, -5.0852078e-01, -1.0347528e+00],\n",
       "       [-9.0651818e-02,  5.5569202e-01, -1.9681559e+00,  1.5604615e+00],\n",
       "       [-1.4446754e+00,  1.8737437e+00, -8.5840929e-01,  2.7800968e-01],\n",
       "       [-7.2376180e-01,  6.5672863e-01, -5.2121948e-03, -2.5962692e-01],\n",
       "       [-6.9333893e-01, -4.2962697e-01,  1.2738094e+00, -2.0840783e-01],\n",
       "       [-1.9882036e+00,  1.9470445e+00, -7.6120192e-01,  8.5731059e-01],\n",
       "       [ 2.0553126e+00, -1.1165123e+00, -2.4368372e+00,  1.1474683e+00],\n",
       "       [ 1.7952882e+00, -1.0295930e+00, -1.4687315e+00,  2.1809448e-01],\n",
       "       [-1.8654697e+00, -2.2038984e+00,  5.4086227e+00, -9.5053875e-01],\n",
       "       [ 5.9075296e-01, -1.9366342e+00,  2.4332862e+00, -1.4687426e+00],\n",
       "       [-5.0563389e-01, -1.3342285e+00,  4.8962390e-01,  1.2540665e+00],\n",
       "       [-6.4036348e-03, -8.5303217e-01, -1.6723143e+00,  2.6699271e+00],\n",
       "       [ 4.9261442e-01, -9.9061167e-01,  3.9191538e-01, -2.5541106e-01],\n",
       "       [ 1.2869259e+00, -1.2034292e+00, -1.6400167e+00,  1.2211915e+00],\n",
       "       [-2.2704732e+00, -1.7740554e+00,  2.1832752e+00,  2.4782836e+00],\n",
       "       [-2.1556487e+00, -8.2476944e-01,  1.7996196e+00,  1.3642287e+00],\n",
       "       [-1.7550834e+00,  2.3116574e+00, -2.5413829e-01, -4.3610692e-01],\n",
       "       [ 3.9573106e-01,  4.1287396e-02, -1.7425652e+00,  1.4463696e+00],\n",
       "       [ 1.1106244e+00, -1.2210649e+00, -7.1152544e-01,  6.9947100e-01],\n",
       "       [ 4.9307793e-01, -2.2362087e+00,  6.8681610e-01,  1.2508074e+00],\n",
       "       [-5.2068651e-01, -7.5364298e-01,  1.5769110e+00, -2.5311786e-01],\n",
       "       [ 5.4720640e-01, -1.8255566e+00,  1.4813693e-01,  8.8955224e-01],\n",
       "       [ 1.4564843e+00, -7.4867681e-02, -2.4574878e+00,  7.8079730e-01],\n",
       "       [ 2.2115669e+00, -1.9193980e+00, -1.3007554e+00,  7.2735238e-01],\n",
       "       [ 2.4549925e+00, -6.6749251e-01, -1.7265576e+00, -5.1022607e-01],\n",
       "       [-1.3622864e+00, -1.1400552e+00,  3.7296276e+00, -8.8633823e-01],\n",
       "       [-1.6736481e+00, -6.2171668e-01,  4.1032710e+00, -1.7264674e+00],\n",
       "       [-5.0537981e-02, -1.0441326e+00,  1.2812248e+00, -4.1585052e-01],\n",
       "       [ 4.8575935e-01, -2.0061820e+00, -1.9052498e-01,  1.6519333e+00],\n",
       "       [-3.2651231e-01, -8.7812269e-01, -1.5046746e+00,  2.9838481e+00],\n",
       "       [ 7.8997332e-01,  7.2159529e-02, -1.2911676e+00,  4.3267706e-01],\n",
       "       [ 3.2810059e-01, -1.8905921e+00,  2.1218083e+00, -4.8606205e-01],\n",
       "       [-1.5804133e+00, -2.1732184e-01, -2.6220438e-01,  2.2763042e+00],\n",
       "       [-2.4794686e-01,  2.2114739e+00, -2.1540091e+00,  3.2139737e-02],\n",
       "       [ 2.3763828e+00, -2.1350207e+00, -2.0798881e+00,  1.8669310e+00],\n",
       "       [ 3.6831620e-01, -2.0546670e+00, -1.9371830e-03,  2.0014081e+00],\n",
       "       [ 2.4845040e+00, -7.0745623e-01, -2.2149534e+00,  2.3036811e-01],\n",
       "       [ 8.3876705e-01, -7.3113716e-01,  1.2885313e-01, -1.0343959e+00],\n",
       "       [-1.9753535e+00, -1.0372850e+00, -6.5296608e-01,  4.0826511e+00],\n",
       "       [-2.2028034e+00,  4.9941996e-01,  2.2902148e+00, -3.2543600e-01],\n",
       "       [-1.2902340e+00, -5.1697671e-01,  2.8804936e+00, -9.9889064e-01],\n",
       "       [-1.1333698e+00,  8.1749654e-01, -2.1374538e+00,  2.6562717e+00],\n",
       "       [ 2.8442748e+00, -7.8864980e-01, -2.5646224e+00,  2.6841062e-01],\n",
       "       [ 3.7966177e+00, -2.3179042e+00, -1.5072337e+00, -2.2484659e-01],\n",
       "       [ 1.6415821e+00, -2.3481908e+00, -7.2218651e-01,  1.3614583e+00],\n",
       "       [ 1.7444541e+00, -1.1432273e+00, -7.4793541e-01, -3.8948134e-01],\n",
       "       [-8.3216506e-01,  2.2642415e+00, -2.6000905e+00,  1.0840757e+00],\n",
       "       [ 1.3267447e+00, -1.4795481e+00, -1.3275835e-01, -1.5125088e-01],\n",
       "       [-8.4741992e-01,  3.6095351e-01,  1.1333804e+00, -4.1023239e-01],\n",
       "       [ 1.9864099e+00, -9.4506508e-01, -1.0071025e+00, -3.3855093e-01]],\n",
       "      dtype=float32), label_ids=None, metrics={'test_runtime': 0.5897, 'test_samples_per_second': 84.791, 'test_steps_per_second': 1.696})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/xlm-emo\",\n",
    "    #learning_rate=2e-5,\n",
    "    #per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    #num_train_epochs=2,\n",
    "    #weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    #train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.predict(tokenized_dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
