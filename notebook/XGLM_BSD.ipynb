{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c17092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 22 09:15:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:4F:00.0 Off |                    0 |\n",
      "| 30%   27C    P8    22W / 300W |      3MiB / 45631MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:52:00.0 Off |                    0 |\n",
      "| 38%   63C    P2   156W / 300W |  25862MiB / 45631MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:56:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    17W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:57:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    22W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:CE:00.0 Off |                    0 |\n",
      "| 30%   28C    P8    20W / 300W |      3MiB / 45631MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:D1:00.0 Off |                    0 |\n",
      "| 30%   31C    P8    18W / 300W |      3MiB / 45631MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:D5:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    22W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:D6:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    21W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A   1202069      C   python                          25859MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebe3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env TOKENIZERS_PARALLELISM=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0266c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XGLMTokenizer, XGLMForCausalLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import tensor \n",
    "import os\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822a5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751d1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-21a84a9e0ce2914d\n",
      "Found cached dataset json (/home/sumire/.cache/huggingface/datasets/json/default-21a84a9e0ce2914d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120146ca8e62438a8e2bdadc55622338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tag', 'title', 'original_language', 'conversation'],\n",
       "        num_rows: 670\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tag', 'title', 'original_language', 'conversation'],\n",
       "        num_rows: 69\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tag', 'title', 'original_language', 'conversation'],\n",
       "        num_rows: 69\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/sumire/discourse_context_mt/data/BSD-master/\"\n",
    "data_files = {\"train\": f\"{file_path}train.json\", \"validation\": f\"{file_path}dev.json\", \"test\": f\"{file_path}test.json\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd168ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> How is it going, Wayne? = ', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> I'm not too bad. = \", 'Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> Thank you very much for coming out today. = ', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> How's business lately? = \", \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> It's been good. = \"] ['はい、K社システム開発部です。', 'H社の高市と申します。', 'いつもお世話になっております。', 'こちらこそ、お世話になっております。', '稲田さんはいらっしゃいますか？']\n"
     ]
    }
   ],
   "source": [
    "# define train inputs and targets\n",
    "\n",
    "inputs = [\"Translate English into Japanese: \"+sent['en_sentence'] for doc in dataset[\"train\"][\"conversation\"] for sent in doc]\n",
    "targets = [sent['ja_sentence'] for doc in dataset[\"train\"][\"conversation\"] for sent in doc]\n",
    "#inputs = [inputs = ['Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> ' + sent['en_sentence'] + \" = \" for doc in dataset[\"test\"][\"conversation\"] for sent in doc ][:50] \n",
    "inputs = ['Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> ' \n",
    "              + sent['en_sentence'] + ' = ' for doc in dataset[\"test\"][\"conversation\"] for sent in doc ][:50] \n",
    "print (inputs[:5], targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4427221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'How is it going, Wayne?'),\n",
       " (2, \"I'm not too bad.\"),\n",
       " (3, 'Thank you very much for coming out today.'),\n",
       " (4, \"How's business lately?\"),\n",
       " (5, \"It's been good.\"),\n",
       " (6,\n",
       "  \"We recently commissioned a new facility so I've been busy managing that.\"),\n",
       " (7, 'I read about that on your company website.'),\n",
       " (8, 'Congratulations.'),\n",
       " (9, 'Thank you.'),\n",
       " (10, 'The Japanese market has been very interested in our product.'),\n",
       " (11,\n",
       "  'So having this new facility should satisfy their demand for the next couple of years.'),\n",
       " (12, 'The Japanese market is trending upwards lately.'),\n",
       " (13,\n",
       "  'I was in Japan last month and talked to a couple of potential customers.'),\n",
       " (14,\n",
       "  'They all told me that they are looking for a reliable and consistent supply.'),\n",
       " (15, 'Well, we are well positioned then.'),\n",
       " (16, 'We have 6 facilities with over 1 million ton capacity.'),\n",
       " (17, 'You also operate your own terminal.'),\n",
       " (18, 'Yes, so we can control the ship loading and schedule.'),\n",
       " (19, 'Having control over the supply chain is definitely helpful.'),\n",
       " (20, 'Being flexible is really important.'),\n",
       " (21, 'How has the Korean market been lately?'),\n",
       " (22, \"I hear that they aren't looking for long term contracts.\"),\n",
       " (23, 'Is that true?'),\n",
       " (24, 'Yes it is.'),\n",
       " (25, 'They are mainly looking for spot contract.'),\n",
       " (26, 'Why is that?'),\n",
       " (27, \"Well they don't want to overcommit to one supplier.\"),\n",
       " (28, 'So they prefer to source from various suppliers.'),\n",
       " (29, 'Is that to also keep the price down?'),\n",
       " (30, 'They can compare prices and shop around for the cheapest price.'),\n",
       " (31, 'Sounds like a lot of work.'),\n",
       " (32,\n",
       "  'We could offer them a 10-year contract and they can sit back and relax.'),\n",
       " (33, 'Yeah, that would make life easier.'),\n",
       " (34, 'We should keep an eye on that market.'),\n",
       " (35, 'Yeah, there is a lot of changes happening lately.'),\n",
       " (36, 'We should catch up again in a couple months?'),\n",
       " (37, 'That would be good.'),\n",
       " (38, 'Great.'),\n",
       " (39, 'Thank you for your time today.'),\n",
       " (40, 'Thank you as well.'),\n",
       " (41, 'Hi there.'),\n",
       " (42, \"I'm Ryan from Company A.\"),\n",
       " (43, \"Hi I'm John from Company B.\"),\n",
       " (44, 'How did you like the seminar just now?'),\n",
       " (45, 'It was okay.'),\n",
       " (46, \"They weren't talking about anything new.\"),\n",
       " (47, 'Is this your first time at this expo?'),\n",
       " (48, 'Yes it is.'),\n",
       " (49, 'That figures.'),\n",
       " (50, 'What do you mean?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [sent['en_sentence'] for doc in dataset[\"test\"][\"conversation\"] for sent in doc][:50]\n",
    "[(index+1, sent) for index, sent in enumerate (inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34d1560",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model_checkpoint = \"facebook/xglm-1.7B\"\n",
    "#model_checkpoint = \"facebook/xglm-2.9B\"\n",
    "#model_checkpoint = \"facebook/xglm-4.5B\"\n",
    "model_checkpoint = \"facebook/xglm-7.5B\"\n",
    "#model_checkpoint = \"bigscience/mt0-small\"\n",
    "#tokenizer = XGLMTokenizer.from_pretrained(model_checkpoint,padding_side = 'left', max_new_tokens=250, max_length=250, truncation=True, padding='max_length')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,padding_side = 'left', truncation=True, padding='max_length')\n",
    "model =  XGLMForCausalLM.from_pretrained(model_checkpoint)\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_function(data): # data should be splitted into train / dev / test internally\n",
    "    \n",
    "    #inputs = [sent['en_sentence']  + \" = \" for doc in data[\"conversation\"] for sent in doc][:50]\n",
    "    \n",
    "    \n",
    "    inputs = ['Hi this is the systems development department of Company K. = はい、K社システム開発部です。 </s> My name is Takaichi from Company H. = H社の高市と申します。</s> Thank you as always. = いつもお世話になっております。 </s> Thank you as always as well = こちらこそ、お世話になっております。</s> ' \n",
    "              + sent['en_sentence'] + ' = ' for doc in data[\"conversation\"] for sent in doc ][:50] \n",
    "    \n",
    "    targets = [sent['ja_sentence'] for doc in data[\"conversation\"] for sent in doc][:50]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=500, truncation=True, padding='max_length'\n",
    "    )\n",
    "    #model_inputs[\"labels\"]\n",
    "    \n",
    "    #model_inputs = tokenizer(inputs,  max_length=250, truncation=True, padding='max_length')\n",
    "    #print (\"input_ids\", model_inputs[\"input_ids\"])\n",
    "    #print (\"labels\", model_inputs[\"labels\"])\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5090232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2e813f15e24a0b84a1e0ee46b01bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6b6decc941488caed7ddb5d57ee52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23cda7446244b808679e97f3437ab31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# Labels*  2, 6, 48245, 39, 443, 3048, 45469, 45648, 1129, 1338, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac03e779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ウェイン、調子はどうです?',\n",
       " 'まあまあです。',\n",
       " '今日はご足労ありがとう。',\n",
       " '景気はどうです?',\n",
       " 'おかげさまで、順調です。',\n",
       " '最近、新しい施設が稼働開始しまして、その管理で忙しくて。',\n",
       " 'ああ、それ、御社のサイトで読みましたよ。',\n",
       " 'おめでとうございます。',\n",
       " 'どうも。',\n",
       " '日本市場が当社製品に興味をもっているようです。',\n",
       " '新施設のおかげで、今後数年間の需要はえるかと思います。',\n",
       " '日本市場は最近上向きだね。',\n",
       " '先月日本に出張して、見込みのありそうな数社とミーティングしてきました。',\n",
       " '全社、声を揃えて、信頼できて一貫性のある供給を望んでいましたね。',\n",
       " 'では、当社はいい状況にあるってわけですね。',\n",
       " '出力百万トン以上の工場が6件ありますし。',\n",
       " '端末も独自に管理されてますしね。',\n",
       " 'そう、出荷量やスケジュールも管理できます。',\n",
       " 'サプライチェーンを管理できるのはとても助かります。',\n",
       " '柔軟性があるのはとても重要です。',\n",
       " '韓国市場は最近どんな状況ですか?',\n",
       " '長期の契約は望んでいないと聞きましたが。',\n",
       " 'そうなんでしょうか?',\n",
       " '本当みたいです。',\n",
       " '現物契約中心のようです。',\n",
       " 'どうしてなんでしょう?',\n",
       " 'サプライヤー一社に傾倒したくないんでしょうね。',\n",
       " 'なので、いろいろなところから調達したいんですよ。',\n",
       " '価格を抑える意味もあるんでしょうか?',\n",
       " '価格を比較して、一番安いところから購入するのです。',\n",
       " 'よっぽど、大変そうですがね。',\n",
       " '10年契約をオファーすれば、向こうはのんびりリラックスできるのに。',\n",
       " 'その方が楽でしょうに。',\n",
       " '韓国市場からは目が離せないですね。',\n",
       " 'そう、最近、いろいろ変化が起きていますからね。',\n",
       " '2ヶ月後くらいにまた、情報交換しましょう。',\n",
       " 'いいですね。',\n",
       " '良かった。',\n",
       " '今日はお時間をありがとうございました。',\n",
       " 'こちらこそ。',\n",
       " 'こんにちは。',\n",
       " 'A社のライアンです。',\n",
       " 'こんにちは、B社のジョンといいます。',\n",
       " 'セミナーはいかがでしたか?',\n",
       " '別に普通でしたね。',\n",
       " '特に目新しいことは何もなかったです。',\n",
       " 'この博覧会は初めてですか?',\n",
       " 'そうです。',\n",
       " 'ああ、それで。',\n",
       " 'といいますと?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized_datasets[\"test\"][\"labels\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c562acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f69492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-da is already in cache.\n",
      "Created a temporary directory at /tmp/tmpk3ltbe6p\n",
      "Writing /tmp/tmpk3ltbe6p/_remote_module_non_scriptable.py\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "metric1 = evaluate.load(\"sacrebleu\")\n",
    "metric2 =  evaluate.load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248c452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels, input_ids):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    input_ids = [[input_id.strip()] for input_id in input_ids]\n",
    "\n",
    "    return preds, labels, input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de726524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1269, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" = \")\n",
    "tokenizer.encode(\"=\")\n",
    "tokenizer.encode(\"= \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56d1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang = \"ja\"\n",
    "output_dir = \"./results/playingaround\"\n",
    "\n",
    "def compute_metrics(dataset, output_dir, tgt_lang, tokenizer, eval_preds):\n",
    "    preds, labels, input_ids = eval_preds\n",
    "    labels = tokenized_datasets[\"test\"][\"labels\"]  # Why We have to define it again ?? \n",
    "    print (\"preds before split:\", tokenizer.batch_decode(preds[:5], skip_special_tokens=True))\n",
    "    #print (\"labels:\", tokenizer.batch_decode(labels[:5], skip_special_tokens=True))\n",
    "    print ()\n",
    "    print (\"input before split:\",tokenizer.batch_decode(input_ids[:5], skip_special_tokens=True))\n",
    "    print ()\n",
    "    \n",
    "    sep = tokenizer.sep_token_id\n",
    "    split_id = tokenizer.encode(\"=\")[-1]\n",
    "    \n",
    "    # Preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds\n",
    "    #preds = [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in preds ]\n",
    "    preds = [ np.array_split(item, np.where(item == split_id)[-1])[-1]  for item in preds ]\n",
    "    del_index= [0, 1] # Delete \"= \" in the beggining of the prediction \n",
    "    preds =[ np.delete(item, del_index) for item in preds ]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    print (\"splited preds: \", decoded_preds[:5])\n",
    "    print ()\n",
    "    print (\"splitted preds ids: \", preds)\n",
    "    print ()\n",
    "    \n",
    "    # Store prediction inference\n",
    "    with open(output_dir+'/translations.txt','w', encoding='utf8') as wf:\n",
    "         for translation, ids in zip(decoded_preds, preds):\n",
    "            wf.write(translation.strip()+'\\n')\n",
    "            wf.write(str(ids)+'\\n')\n",
    "    \n",
    "    \n",
    "    # Labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    #print (\"labels from eval_preds\", [tokenizer.decode(i, skip_special_tokens=True) for i in labels])\n",
    "    print ()\n",
    "    #labels= [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in labels ]\n",
    "    #print (\"checking labels_token:\")\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    print (\"splitted labels from dataset: \", decoded_labels[:5])\n",
    "    print ()\n",
    "    \n",
    "    # Input_ids\n",
    "    #input_ids = np.where(input_ids != -100, input_ids, tokenizer.pad_token_id)\n",
    "    input_ids = [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in input_ids ]\n",
    "    input_ids = [ np.array_split(item, np.where(item == split_id)[-1])[0]  for item in input_ids ] \n",
    "    decoded_input_ids = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    print (\"splited input_ids\", decoded_input_ids[:5])\n",
    "    print ()\n",
    "    \n",
    "\n",
    "    decoded_preds, decoded_labels, decoded_input_ids = postprocess_text(decoded_preds, decoded_labels, decoded_input_ids)\n",
    "    \n",
    "    # bleu\n",
    "    if tgt_lang == \"ja\":\n",
    "        bleu = metric1.compute(predictions=decoded_preds, references=decoded_labels, tokenize='ja-mecab')\n",
    "    else: \n",
    "        bleu = metric1.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": bleu[\"score\"]}\n",
    "\n",
    "    # comet\n",
    "    #print (\"decoded_input_ids:\",  [item for decoded_input_id in decoded_input_ids for item in decoded_input_id][:5], \"\\ndecoded_preds\", decoded_preds[:5], \"\\ndecoded_labels\", [item for decoded_label in decoded_labels for item in decoded_label][:5])\n",
    "    \n",
    "    comet = metric2.compute(predictions=decoded_preds, references=[item for decoded_label in decoded_labels for item in decoded_label], sources = [item for decoded_input_id in decoded_input_ids for item in decoded_input_id])\n",
    "    print (\"comet\")\n",
    "    print (\"decoded_preds\", decoded_preds)\n",
    "    print()\n",
    "    print (\"references\", [item for decoded_label in decoded_labels for item in decoded_label])\n",
    "    print()\n",
    "    print (\"source\",  [item for decoded_input_id in decoded_input_ids for item in decoded_input_id])\n",
    "    result[\"comet\"] =  np.mean(comet[\"scores\"])\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(result)\n",
    "\n",
    "    # Store the score\n",
    "    with open(output_dir+'/test_score.txt','w', encoding='utf8') as wf:\n",
    "        for key, value in result.items():\n",
    "            wf.write(f\"{key}: {value}\\n\") #ensure_ascii=False\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5772772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "You're using a XGLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "/home/sumire/miniconda3/envs/llm_mt/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds before split: ['Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 How is it going, Wayne? = ウ', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 I'm not too bad. = そこ\", 'Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 Thank you very much for coming out today. = いつも', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 How's business lately? = ビジネス\", \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 It's been good. = いつも\"]\n",
      "\n",
      "input before split: ['Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 How is it going, Wayne? = ', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 I'm not too bad. = \", 'Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 Thank you very much for coming out today. = ', \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 How's business lately? = \", \"Hi this is the systems development department of Company K. = はい、K社システム開発部です。  My name is Takaichi from Company H. = H社の高市と申します。 Thank you as always. = いつもお世話になっております。  Thank you as always as well = こちらこそ、お世話になっております。 It's been good. = \"]\n",
      "\n",
      "splited preds:  ['ウ', 'そこ', 'いつも', 'ビジネス', 'いつも']\n",
      "\n",
      "splitted preds ids:  [array([4032]), array([24346]), array([31261]), array([77182]), array([31261]), array([85967]), array([14798]), array([21095]), array([74823]), array([44753]), array([17988]), array([44753]), array([15836]), array([10880]), array([48245]), array([85967]), array([18862]), array([48245]), array([2081]), array([13887]), array([42678]), array([13450]), array([80710]), array([48245]), array([74002]), array([50084]), array([10880]), array([10880]), array([26731]), array([10880]), array([42526]), array([10880]), array([48245]), array([14481]), array([48245]), array([2157]), array([34685]), array([48245]), array([18862]), array([31261]), array([48245]), array([35544]), array([48245]), array([11019]), array([34685]), array([10880]), array([37005]), array([48245]), array([34685]), array([134550])]\n",
      "\n",
      "\n",
      "splitted labels from dataset:  ['ウェイン、調子はどうです?', 'まあまあです。', '今日はご足労ありがとう。', '景気はどうです?', 'おかげさまで、順調です。']\n",
      "\n",
      "splited input_ids ['How is it going, Wayne?', \"I'm not too bad.\", 'Thank you very much for coming out today.', \"How's business lately?\", \"It's been good.\"]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comet\n",
      "decoded_preds ['ウ', 'そこ', 'いつも', 'ビジネス', 'いつも', '私たち', '会社', '御', 'ありがとうございます', '日本の', 'こう', '日本の', '去年', '彼', 'はい', '私たち', '貴', 'はい', '流', '柔', '韓国', '聞', '本当', 'はい', '基本的', 'なぜ', '彼', '彼', '価格', '彼', '大変', '彼', 'はい', '市場', 'はい', '次', 'それは', 'はい', '貴', 'いつも', 'はい', '私の', 'はい', '講', 'それは', '彼', '初めて', 'はい', 'それは', 'なに']\n",
      "\n",
      "references ['ウェイン、調子はどうです?', 'まあまあです。', '今日はご足労ありがとう。', '景気はどうです?', 'おかげさまで、順調です。', '最近、新しい施設が稼働開始しまして、その管理で忙しくて。', 'ああ、それ、御社のサイトで読みましたよ。', 'おめでとうございます。', 'どうも。', '日本市場が当社製品に興味をもっているようです。', '新施設のおかげで、今後数年間の需要はえるかと思います。', '日本市場は最近上向きだね。', '先月日本に出張して、見込みのありそうな数社とミーティングしてきました。', '全社、声を揃えて、信頼できて一貫性のある供給を望んでいましたね。', 'では、当社はいい状況にあるってわけですね。', '出力百万トン以上の工場が6件ありますし。', '端末も独自に管理されてますしね。', 'そう、出荷量やスケジュールも管理できます。', 'サプライチェーンを管理できるのはとても助かります。', '柔軟性があるのはとても重要です。', '韓国市場は最近どんな状況ですか?', '長期の契約は望んでいないと聞きましたが。', 'そうなんでしょうか?', '本当みたいです。', '現物契約中心のようです。', 'どうしてなんでしょう?', 'サプライヤー一社に傾倒したくないんでしょうね。', 'なので、いろいろなところから調達したいんですよ。', '価格を抑える意味もあるんでしょうか?', '価格を比較して、一番安いところから購入するのです。', 'よっぽど、大変そうですがね。', '10年契約をオファーすれば、向こうはのんびりリラックスできるのに。', 'その方が楽でしょうに。', '韓国市場からは目が離せないですね。', 'そう、最近、いろいろ変化が起きていますからね。', '2ヶ月後くらいにまた、情報交換しましょう。', 'いいですね。', '良かった。', '今日はお時間をありがとうございました。', 'こちらこそ。', 'こんにちは。', 'A社のライアンです。', 'こんにちは、B社のジョンといいます。', 'セミナーはいかがでしたか?', '別に普通でしたね。', '特に目新しいことは何もなかったです。', 'この博覧会は初めてですか?', 'そうです。', 'ああ、それで。', 'といいますと?']\n",
      "\n",
      "source ['How is it going, Wayne?', \"I'm not too bad.\", 'Thank you very much for coming out today.', \"How's business lately?\", \"It's been good.\", \"We recently commissioned a new facility so I've been busy managing that.\", 'I read about that on your company website.', 'Congratulations.', 'Thank you.', 'The Japanese market has been very interested in our product.', 'So having this new facility should satisfy their demand for the next couple of years.', 'The Japanese market is trending upwards lately.', 'I was in Japan last month and talked to a couple of potential customers.', 'They all told me that they are looking for a reliable and consistent supply.', 'Well, we are well positioned then.', 'We have 6 facilities with over 1 million ton capacity.', 'You also operate your own terminal.', 'Yes, so we can control the ship loading and schedule.', 'Having control over the supply chain is definitely helpful.', 'Being flexible is really important.', 'How has the Korean market been lately?', \"I hear that they aren't looking for long term contracts.\", 'Is that true?', 'Yes it is.', 'They are mainly looking for spot contract.', 'Why is that?', \"Well they don't want to overcommit to one supplier.\", 'So they prefer to source from various suppliers.', 'Is that to also keep the price down?', 'They can compare prices and shop around for the cheapest price.', 'Sounds like a lot of work.', 'We could offer them a 10-year contract and they can sit back and relax.', 'Yeah, that would make life easier.', 'We should keep an eye on that market.', 'Yeah, there is a lot of changes happening lately.', 'We should catch up again in a couple months?', 'That would be good.', 'Great.', 'Thank you for your time today.', 'Thank you as well.', 'Hi there.', \"I'm Ryan from Company A.\", \"Hi I'm John from Company B.\", 'How did you like the seminar just now?', 'It was okay.', \"They weren't talking about anything new.\", 'Is this your first time at this expo?', 'Yes it is.', 'That figures.', 'What do you mean?']\n",
      "{'bleu': 0.0, 'comet': -1.054, 'gen_len': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.142801523208618,\n",
       " 'eval_bleu': 0.0,\n",
       " 'eval_comet': -1.054,\n",
       " 'eval_gen_len': 1.0,\n",
       " 'eval_runtime': 45.2179,\n",
       " 'eval_samples_per_second': 1.106,\n",
       " 'eval_steps_per_second': 0.287}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results/playingaround\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_eval_batch_size=4, \n",
    "    do_predict=True,\n",
    "    do_eval = True,\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=True,\n",
    "    warmup_steps = 500,\n",
    "    #metric_for_best_model = \"comet\"\n",
    "    #load_best_model_at_end = True,\n",
    "    greater_is_better = True,\n",
    "    #save_strategy = \"steps\",\n",
    "    eval_delay = 0.0,\n",
    "    eval_accumulation_steps = 20\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=partial(compute_metrics, dataset, output_dir, tgt_lang, tokenizer),\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514b126",
   "metadata": {},
   "source": [
    "Findings \n",
    "- To translate, specific prompt template is needed: \"{Source sentence} = {Target sentence}\", otherwise performance  \n",
    "- zero shot is not working\n",
    "- From one shot it shows some correct translation (but with only a token)\n",
    "- We can split the prompt and prediction, also input, by \"=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e662b4",
   "metadata": {},
   "source": [
    "Question \n",
    "- Input length of input_ids is 500, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
    "- Padding and Truncation?\n",
    "- Labels are not handed over to compute_metrics function, inside the eval_preds (At the moment I directly manually put the reference from dataset)\n",
    "- Why the model predits only one token ? (But the token is good one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
