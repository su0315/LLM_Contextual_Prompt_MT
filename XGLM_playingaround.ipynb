{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c17092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 10 14:34:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:4F:00.0 Off |                    0 |\n",
      "| 30%   49C    P2   149W / 300W |  43748MiB / 45631MiB |     38%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:52:00.0 Off |                    0 |\n",
      "| 30%   58C    P2   162W / 300W |  41964MiB / 45631MiB |     46%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:56:00.0 Off |                  Off |\n",
      "| 30%   50C    P2   154W / 300W |  43426MiB / 48685MiB |     37%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:57:00.0 Off |                  Off |\n",
      "| 30%   56C    P2   162W / 300W |  46484MiB / 48685MiB |     36%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:CE:00.0 Off |                    0 |\n",
      "| 41%   67C    P2   280W / 300W |  20922MiB / 45631MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:D1:00.0 Off |                    0 |\n",
      "| 51%   76C    P2   286W / 300W |  20236MiB / 45631MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:D5:00.0 Off |                  Off |\n",
      "| 39%   67C    P2   256W / 300W |  20760MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:D6:00.0 Off |                  Off |\n",
      "| 50%   76C    P2   259W / 300W |  20518MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2843471      C   python                          43745MiB |\n",
      "|    1   N/A  N/A   2843171      C   python                          41961MiB |\n",
      "|    2   N/A  N/A   2838691      C   python                          43423MiB |\n",
      "|    3   N/A  N/A   2838456      C   python                          46481MiB |\n",
      "|    4   N/A  N/A   2427572      C   ...irtualenvs/gpt/bin/python    20919MiB |\n",
      "|    5   N/A  N/A   2427573      C   ...irtualenvs/gpt/bin/python    20233MiB |\n",
      "|    6   N/A  N/A   2427574      C   ...irtualenvs/gpt/bin/python    20757MiB |\n",
      "|    7   N/A  N/A   2427575      C   ...irtualenvs/gpt/bin/python    20515MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0266c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XGLMTokenizer, XGLMForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch import tensor \n",
    "import os\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/sumire/discourse_context_mt/data/BSD-master/\"\n",
    "data_files = {\"train\": f\"{file_path}train.json\", \"validation\": f\"{file_path}dev.json\", \"test\": f\"{file_path}test.json\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd168ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train inputs and targets\n",
    "\n",
    "inputs = [\"Translate English into Japanese: \"+sent['en_sentence'] for doc in dataset[\"train\"][\"conversation\"] for sent in doc]\n",
    "targets = [sent['ja_sentence'] for doc in dataset[\"train\"][\"conversation\"] for sent in doc]\n",
    "\n",
    "print (inputs[:5], targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/xglm-7.5B\"\n",
    "configuration = BloomConfig()\n",
    "tokenizer = XGLMTokenizer.from_pretrained(model_checkpoint)\n",
    "model =  XGLMForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_function(data): # data should be splitted into train / dev / test internally\n",
    "    inputs = [\"Translate English to Japanese: \"+sent['en_sentence'] for doc in data[\"conversation\"] for sent in doc][:50]\n",
    "    targets = [sent['ja_sentence'] for doc in data[\"conversation\"] for sent in doc][:50]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=128, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5090232",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-7.5B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f69492",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = evaluate.load(\"sacrebleu\")\n",
    "metric2 =  evaluate.load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels, input_ids):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    input_ids = [[input_id.strip()] for input_id in input_ids]\n",
    "\n",
    "    return preds, labels, input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(output_dir, tgt_lang = \"ja\", tokenizer, eval_preds):\n",
    "    preds, labels, input_ids = eval_preds # Check the location of input_ids is appropriate\n",
    "    \n",
    "    # Preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    #sep = tokenizer.sep_token_id\n",
    "    #preds = [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in preds ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    print (\"decoded_preds: \", decoded_preds[:5])\n",
    "    #with open('./results/bsd_en-ja/bleu_ja_pred/inference.json', 'w', encoding='utf8') as json_file:\n",
    "        #json.dump(decoded_preds, json_file, ensure_ascii=False,)\n",
    "    \n",
    "    # Store inference\n",
    "    with open(output_dir+'/translations.txt','w', encoding='utf8') as wf:\n",
    "         for translation in decoded_preds:\n",
    "            wf.write(translation.strip()+'\\n') \n",
    "\n",
    "    #Labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    #labels= [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in labels ]\n",
    "    #print (\"checking labels_token:\")\n",
    "    #print (labels[:10][:5])\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    print (\"decoded_labels:\", decoded_labels[:5])\n",
    "\n",
    "    \n",
    "    # Input_ids\n",
    "    # For comet source info\n",
    "    input_ids = np.where(input_ids != -100, input_ids, tokenizer.pad_token_id)\n",
    "    #print (\"checking input_ids before split:\", input_ids[:10][:5])\n",
    "    #input_ids = [ np.array_split(item, np.where(item == sep)[-1])[-1]  for item in input_ids ]\n",
    "    #print (\"checking input_ids3 after split:\")\n",
    "    #print (input_ids[:10][:5])\n",
    "    decoded_input_ids = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    decoded_preds, decoded_labels, decoded_input_ids = postprocess_text(decoded_preds, decoded_labels, decoded_input_ids)\n",
    "    \n",
    "    # bleu\n",
    "    if tgt_lang == \"ja\":\n",
    "        bleu = metric1.compute(predictions=decoded_preds, references=decoded_labels, tokenize='ja-mecab')\n",
    "    else: \n",
    "        bleu = metric1.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": bleu[\"score\"]}\n",
    "\n",
    "    # comet\n",
    "    print (\"decoded_input_ids:\",  [item for decoded_input_id in decoded_input_ids for item in decoded_input_id][:5], \"\\ndecoded_preds\", decoded_preds[:5], \"\\ndecoded_labels\", [item for decoded_label in decoded_labels for item in decoded_label][:5])\n",
    "    \n",
    "    comet = metric2.compute(predictions=decoded_preds, references=[item for decoded_label in decoded_labels for item in decoded_label], sources = [item for decoded_input_id in decoded_input_ids for item in decoded_input_id])\n",
    "    result[\"comet\"] =  np.mean(comet[\"scores\"])\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(result)\n",
    "\n",
    "    # Store the score\n",
    "    with open(output_dir+'/test_score.txt','w', encoding='utf8') as wf:\n",
    "        for key, value in result.items():\n",
    "            wf.write(f\"{key}: {value}\\n\") #ensure_ascii=False\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5772772",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/playingaround\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=partial(eval_bleu.compute_metrics, output_dir, tgt_lang, tokenizer),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
